{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e0869b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[:int(n * 0.8)]\n",
    "        \n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[:int(n * 0.8)]\n",
    "        \n",
    "    elif split == 'val':\n",
    "        f_in = ROOT_PATH + 'train' + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[int(n * 0.8):]\n",
    "        \n",
    "        f_out = ROOT_PATH + 'train' + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[int(n * 0.8):]\n",
    "    \n",
    "    elif split == 'test':\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)\n",
    "\n",
    "    if normalized:\n",
    "        ## Normalize based on initial input coordinates\n",
    "        ## Loop through agents\n",
    "        all_normalized_inp = np.zeros((inputs.shape[0], 50, 2))\n",
    "        all_normalized_out = np.zeros((inputs.shape[0], 60, 2))\n",
    "        for i in range(len(inputs)):\n",
    "            dat = inputs[i]\n",
    "            x_0 = dat[0][0]\n",
    "            y_0 = dat[0][1]\n",
    "            temp_x = (dat[:, 0] - x_0)\n",
    "            temp_y = (dat[:, 1] - y_0)\n",
    "            normalized_inp = np.dstack([temp_x, temp_y])\n",
    "            all_normalized_inp[i] = normalized_inp\n",
    "            \n",
    "            if split != 'test':\n",
    "                dat = outputs[i]\n",
    "                x_0 = dat[0][0]\n",
    "                y_0 = dat[0][1]\n",
    "                temp_x = (dat[:, 0] - x_0)\n",
    "                temp_y = (dat[:, 1] - y_0)\n",
    "                normalized_out = np.dstack([temp_x, temp_y])\n",
    "                all_normalized_out[i] = normalized_out\n",
    "    inputs = all_normalized_inp\n",
    "    if split != 'test':\n",
    "        outputs = all_normalized_out\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.outputs is not None:\n",
    "            data = (self.inputs[idx], self.outputs[idx])\n",
    "        else:\n",
    "            data = self.inputs[idx]\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'palo-alto' \n",
    "split = 'train'\n",
    "train_dataset  = ArgoverseDataset(city = city, split = split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 32  # batch size \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import random\n",
    "\n",
    "# def show_sample_batch(sample_batch):\n",
    "#     \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "#     inp, out = sample_batch\n",
    "#     batch_sz = inp.size(0)\n",
    "#     agent_sz = inp.size(1)\n",
    "    \n",
    "#     fig, axs = plt.subplots(1, batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "#     fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "#     axs = axs.ravel()   \n",
    "#     for i in range(batch_sz):\n",
    "#         axs[i].xaxis.set_ticks([])\n",
    "#         axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "#         axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "#         axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "\n",
    "        \n",
    "# for i_batch, sample_batch in enumerate(train_loader):\n",
    "#     # inp[i] is a scene with 50 coordinates, input[i, j] is a coordinate\n",
    "#     # gotta loop through each scene in the batch\n",
    "#     inp, out = sample_batch # inp: (batch size, 50, 2), out: (batch size, 60, 2)\n",
    "#     \"\"\"\n",
    "#     TODO:\n",
    "#       implement your Deep learning model\n",
    "#       implement training routine\n",
    "#     \"\"\"\n",
    "#     show_sample_batch(sample_batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54581fd-e85e-4f05-ad67-375a897d3580",
   "metadata": {},
   "source": [
    "## I guess SKLearn is our savior :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc84bf0-950a-4b1b-805a-3cd5e979a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f5560c-40cf-4c40-a28f-9e99de00879a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 22.469596603067284\n",
      "Validation Loss: 23.394195340919037\n",
      "Training Loss: 19.01659043726902\n",
      "Validation Loss: 19.13593787667287\n",
      "Training Loss: 19.100396399152963\n",
      "Validation Loss: 19.622918344000684\n",
      "Training Loss: 23.703932780832538\n",
      "Validation Loss: 23.53079824564754\n",
      "Training Loss: 20.71187504473186\n",
      "Validation Loss: 22.01960472105625\n",
      "Training Loss: 23.155429901404535\n",
      "Validation Loss: 25.237217393136\n"
     ]
    }
   ],
   "source": [
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"] \n",
    "header = ['ID'] + ['v' + str(i) for i in range(0, 120)]\n",
    "df = pd.DataFrame(columns=header)\n",
    "\n",
    "for city in cities:\n",
    "    output = ''\n",
    "    x_train, y_train = get_city_trajectories(city = city, split='train')\n",
    "    x_val, y_val = get_city_trajectories(city = city, split='val')\n",
    "    x_test, y_test = get_city_trajectories(city = city, split='test')\n",
    "    \n",
    "    # Reshape for model\n",
    "    x_train = x_train.reshape(-1,100)\n",
    "    y_train = y_train.reshape(-1,120)\n",
    "    x_test = x_test.reshape(-1,100)\n",
    "    x_val = x_val.reshape(-1,100)\n",
    "    y_val = y_val.reshape(-1,120)\n",
    "    \n",
    "    ## Fit model\n",
    "    lr = LinearRegression().fit(x_train, y_train)\n",
    "    \n",
    "    ## Try MLP???\n",
    "    #mlp = MLPClassifier(random_state=1, max_iter=300).fit(x_train, y_train)\n",
    "    \n",
    "    ## Train Set\n",
    "    train_preds = lr.predict(x_train)\n",
    "    train_preds = torch.from_numpy(train_preds)\n",
    "    y_out = torch.from_numpy(y_train)\n",
    "    \n",
    "    ## Validation Set\n",
    "    val_preds = torch.from_numpy(lr.predict(x_val))\n",
    "    val_out = torch.from_numpy(y_val)\n",
    "    loss_fct = nn.MSELoss()\n",
    "    \n",
    "    print('Training Loss: {}'.format(loss_fct(train_preds, y_out).item()))\n",
    "    print('Validation Loss: {}'.format(loss_fct(val_preds, val_out).item()))\n",
    "\n",
    "    \n",
    "    ## Predictions + Write Submission\n",
    "    preds = lr.predict(x_test)\n",
    "    indices = range(len(x_test))\n",
    "    row = ['{}_{}'.format(i, city) for i in indices]\n",
    "    output += ','.join(row) + '\\n'\n",
    "    try:\n",
    "        with open('./submission.csv', 'a') as f:\n",
    "            f.write(output)\n",
    "    except:\n",
    "        print('Error! Unsuccessful write...')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9857e67-dc66-4628-977a-e0a0d8a25bf5",
   "metadata": {},
   "source": [
    "## OR IS PYTORCH!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c4ac534-895c-4699-b918-a263109cbb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, training loss: 109.05902099609375, validation loss: 120.27942657470703\n",
      "epoch: 2, training loss: 100.95133972167969, validation loss: 108.87681579589844\n",
      "epoch: 3, training loss: 97.2458724975586, validation loss: 104.93876647949219\n",
      "epoch: 4, training loss: 93.10277557373047, validation loss: 100.39427185058594\n",
      "epoch: 5, training loss: 88.6928482055664, validation loss: 95.49293518066406\n",
      "epoch: 6, training loss: 84.19279479980469, validation loss: 90.42538452148438\n",
      "epoch: 7, training loss: 79.76187133789062, validation loss: 85.36431884765625\n",
      "epoch: 8, training loss: 75.53534698486328, validation loss: 80.46369934082031\n",
      "epoch: 9, training loss: 71.61439514160156, validation loss: 75.84564971923828\n",
      "epoch: 10, training loss: 68.0610580444336, validation loss: 71.59209442138672\n",
      "epoch: 11, training loss: 64.90050506591797, validation loss: 67.74463653564453\n",
      "epoch: 12, training loss: 62.128639221191406, validation loss: 64.31094360351562\n",
      "epoch: 13, training loss: 59.72176742553711, validation loss: 61.27461624145508\n",
      "epoch: 14, training loss: 57.64552688598633, validation loss: 58.6049690246582\n",
      "epoch: 15, training loss: 55.861488342285156, validation loss: 56.26476287841797\n",
      "epoch: 16, training loss: 54.331485748291016, validation loss: 54.215370178222656\n",
      "epoch: 17, training loss: 53.01991271972656, validation loss: 52.41987228393555\n",
      "epoch: 18, training loss: 51.8947868347168, validation loss: 50.844512939453125\n",
      "epoch: 19, training loss: 50.92804718017578, validation loss: 49.45928955078125\n",
      "epoch: 20, training loss: 50.09540557861328, validation loss: 48.23790740966797\n",
      "epoch: 21, training loss: 49.376033782958984, validation loss: 47.157562255859375\n",
      "epoch: 22, training loss: 48.7521858215332, validation loss: 46.19859313964844\n",
      "epoch: 23, training loss: 48.208824157714844, validation loss: 45.34410858154297\n",
      "epoch: 24, training loss: 47.73324203491211, validation loss: 44.57965850830078\n",
      "epoch: 25, training loss: 47.31473159790039, validation loss: 43.89286804199219\n",
      "epoch: 26, training loss: 46.944297790527344, validation loss: 43.273197174072266\n",
      "epoch: 27, training loss: 46.61439514160156, validation loss: 42.71162414550781\n",
      "epoch: 28, training loss: 46.318695068359375, validation loss: 42.20046615600586\n",
      "epoch: 29, training loss: 46.05192184448242, validation loss: 41.73316192626953\n",
      "epoch: 30, training loss: 45.80964279174805, validation loss: 41.304100036621094\n",
      "epoch: 31, training loss: 45.588157653808594, validation loss: 40.908485412597656\n",
      "epoch: 32, training loss: 45.384376525878906, validation loss: 40.54222106933594\n",
      "epoch: 33, training loss: 45.19571304321289, validation loss: 40.201778411865234\n",
      "epoch: 34, training loss: 45.02000427246094, validation loss: 39.884132385253906\n",
      "epoch: 35, training loss: 44.85543441772461, validation loss: 39.586673736572266\n",
      "epoch: 36, training loss: 44.70048141479492, validation loss: 39.307167053222656\n",
      "epoch: 37, training loss: 44.55387496948242, validation loss: 39.04365921020508\n",
      "epoch: 38, training loss: 44.41454315185547, validation loss: 38.79447937011719\n",
      "epoch: 39, training loss: 44.281578063964844, validation loss: 38.55815505981445\n",
      "epoch: 40, training loss: 44.154212951660156, validation loss: 38.333412170410156\n",
      "epoch: 41, training loss: 44.03181457519531, validation loss: 38.119144439697266\n",
      "epoch: 42, training loss: 43.91383743286133, validation loss: 37.914371490478516\n",
      "epoch: 43, training loss: 43.799808502197266, validation loss: 37.718238830566406\n",
      "epoch: 44, training loss: 43.6893424987793, validation loss: 37.529998779296875\n",
      "epoch: 45, training loss: 43.58209228515625, validation loss: 37.3489875793457\n",
      "epoch: 46, training loss: 43.47777557373047, validation loss: 37.17461395263672\n",
      "epoch: 47, training loss: 43.37614440917969, validation loss: 37.00636672973633\n",
      "epoch: 48, training loss: 43.2769889831543, validation loss: 36.84377670288086\n",
      "epoch: 49, training loss: 43.180118560791016, validation loss: 36.686424255371094\n",
      "epoch: 50, training loss: 43.08536911010742, validation loss: 36.53395462036133\n",
      "epoch: 51, training loss: 42.99260711669922, validation loss: 36.38603210449219\n",
      "epoch: 52, training loss: 42.901702880859375, validation loss: 36.24236297607422\n",
      "epoch: 53, training loss: 42.81256103515625, validation loss: 36.10267639160156\n",
      "epoch: 54, training loss: 42.72507095336914, validation loss: 35.966732025146484\n",
      "epoch: 55, training loss: 42.63915252685547, validation loss: 35.834312438964844\n",
      "epoch: 56, training loss: 42.55473709106445, validation loss: 35.705230712890625\n",
      "epoch: 57, training loss: 42.47175216674805, validation loss: 35.57929229736328\n",
      "epoch: 58, training loss: 42.39012908935547, validation loss: 35.456336975097656\n",
      "epoch: 59, training loss: 42.30983352661133, validation loss: 35.33622360229492\n",
      "epoch: 60, training loss: 42.23079299926758, validation loss: 35.218814849853516\n",
      "epoch: 61, training loss: 42.15297317504883, validation loss: 35.10396957397461\n",
      "epoch: 62, training loss: 42.076324462890625, validation loss: 34.99159622192383\n",
      "epoch: 63, training loss: 42.000816345214844, validation loss: 34.88157653808594\n",
      "epoch: 64, training loss: 41.926414489746094, validation loss: 34.77381134033203\n",
      "epoch: 65, training loss: 41.85307312011719, validation loss: 34.668216705322266\n",
      "epoch: 66, training loss: 41.7807731628418, validation loss: 34.564701080322266\n",
      "epoch: 67, training loss: 41.7094841003418, validation loss: 34.46318817138672\n",
      "epoch: 68, training loss: 41.63916778564453, validation loss: 34.363609313964844\n",
      "epoch: 69, training loss: 41.56980895996094, validation loss: 34.26589584350586\n",
      "epoch: 70, training loss: 41.50138473510742, validation loss: 34.16997528076172\n",
      "epoch: 71, training loss: 41.43386459350586, validation loss: 34.07579803466797\n",
      "epoch: 72, training loss: 41.36722946166992, validation loss: 33.98331069946289\n",
      "epoch: 73, training loss: 41.30146408081055, validation loss: 33.892452239990234\n",
      "epoch: 74, training loss: 41.23653793334961, validation loss: 33.80317306518555\n",
      "epoch: 75, training loss: 41.17243957519531, validation loss: 33.71543884277344\n",
      "epoch: 76, training loss: 41.10914611816406, validation loss: 33.629188537597656\n",
      "epoch: 77, training loss: 41.0466423034668, validation loss: 33.544395446777344\n",
      "epoch: 78, training loss: 40.98491287231445, validation loss: 33.46100616455078\n",
      "epoch: 79, training loss: 40.92393493652344, validation loss: 33.378990173339844\n",
      "epoch: 80, training loss: 40.86370086669922, validation loss: 33.298309326171875\n",
      "epoch: 81, training loss: 40.80419158935547, validation loss: 33.21893310546875\n",
      "epoch: 82, training loss: 40.74539566040039, validation loss: 33.14082336425781\n",
      "epoch: 83, training loss: 40.68729019165039, validation loss: 33.06394958496094\n",
      "epoch: 84, training loss: 40.62986755371094, validation loss: 32.98828125\n",
      "epoch: 85, training loss: 40.57311248779297, validation loss: 32.913795471191406\n",
      "epoch: 86, training loss: 40.51701736450195, validation loss: 32.84046173095703\n",
      "epoch: 87, training loss: 40.46156692504883, validation loss: 32.76824951171875\n",
      "epoch: 88, training loss: 40.406742095947266, validation loss: 32.69713592529297\n",
      "epoch: 89, training loss: 40.352542877197266, validation loss: 32.62709426879883\n",
      "epoch: 90, training loss: 40.29894256591797, validation loss: 32.558101654052734\n",
      "epoch: 91, training loss: 40.245941162109375, validation loss: 32.49013900756836\n",
      "epoch: 92, training loss: 40.19352340698242, validation loss: 32.42317581176758\n",
      "epoch: 93, training loss: 40.14168167114258, validation loss: 32.35719299316406\n",
      "epoch: 94, training loss: 40.09040069580078, validation loss: 32.292179107666016\n",
      "epoch: 95, training loss: 40.039676666259766, validation loss: 32.22810363769531\n",
      "epoch: 96, training loss: 39.98948669433594, validation loss: 32.164947509765625\n",
      "epoch: 97, training loss: 39.93983840942383, validation loss: 32.10269546508789\n",
      "epoch: 98, training loss: 39.890708923339844, validation loss: 32.04132843017578\n",
      "epoch: 99, training loss: 39.842098236083984, validation loss: 31.980823516845703\n",
      "epoch: 100, training loss: 39.79399108886719, validation loss: 31.92116928100586\n",
      "Predictions for austin generated!\n",
      "Done for austin\n",
      "epoch: 1, training loss: 44.4182014465332, validation loss: 26.795249938964844\n",
      "epoch: 2, training loss: 44.29377746582031, validation loss: 26.66801643371582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, training loss: 44.19803237915039, validation loss: 26.55101203918457\n",
      "epoch: 4, training loss: 44.117652893066406, validation loss: 26.440778732299805\n",
      "epoch: 5, training loss: 44.04559326171875, validation loss: 26.335674285888672\n",
      "epoch: 6, training loss: 43.97789764404297, validation loss: 26.23484230041504\n",
      "epoch: 7, training loss: 43.912315368652344, validation loss: 26.137781143188477\n",
      "epoch: 8, training loss: 43.84754180908203, validation loss: 26.04416275024414\n",
      "epoch: 9, training loss: 43.78285217285156, validation loss: 25.95372772216797\n",
      "epoch: 10, training loss: 43.717864990234375, validation loss: 25.866268157958984\n",
      "epoch: 11, training loss: 43.65241622924805, validation loss: 25.781574249267578\n",
      "epoch: 12, training loss: 43.586448669433594, validation loss: 25.699478149414062\n",
      "epoch: 13, training loss: 43.519989013671875, validation loss: 25.619800567626953\n",
      "epoch: 14, training loss: 43.45310974121094, validation loss: 25.542383193969727\n",
      "epoch: 15, training loss: 43.38589859008789, validation loss: 25.467079162597656\n",
      "epoch: 16, training loss: 43.31844711303711, validation loss: 25.39373779296875\n",
      "epoch: 17, training loss: 43.25086212158203, validation loss: 25.322237014770508\n",
      "epoch: 18, training loss: 43.18323516845703, validation loss: 25.252452850341797\n",
      "epoch: 19, training loss: 43.11565017700195, validation loss: 25.184280395507812\n",
      "epoch: 20, training loss: 43.04819107055664, validation loss: 25.117618560791016\n",
      "epoch: 21, training loss: 42.980926513671875, validation loss: 25.052371978759766\n",
      "epoch: 22, training loss: 42.91392517089844, validation loss: 24.988462448120117\n",
      "epoch: 23, training loss: 42.84724807739258, validation loss: 24.925806045532227\n",
      "epoch: 24, training loss: 42.78093719482422, validation loss: 24.864343643188477\n",
      "epoch: 25, training loss: 42.71503829956055, validation loss: 24.80400848388672\n",
      "epoch: 26, training loss: 42.64957809448242, validation loss: 24.7447452545166\n",
      "epoch: 27, training loss: 42.58460235595703, validation loss: 24.686498641967773\n",
      "epoch: 28, training loss: 42.520118713378906, validation loss: 24.62922477722168\n",
      "epoch: 29, training loss: 42.4561653137207, validation loss: 24.572879791259766\n",
      "epoch: 30, training loss: 42.39274597167969, validation loss: 24.517423629760742\n",
      "epoch: 31, training loss: 42.329872131347656, validation loss: 24.46282386779785\n",
      "epoch: 32, training loss: 42.26757049560547, validation loss: 24.409046173095703\n",
      "epoch: 33, training loss: 42.20584487915039, validation loss: 24.35605812072754\n",
      "epoch: 34, training loss: 42.144683837890625, validation loss: 24.303831100463867\n",
      "epoch: 35, training loss: 42.08409881591797, validation loss: 24.252344131469727\n",
      "epoch: 36, training loss: 42.02410888671875, validation loss: 24.201568603515625\n",
      "epoch: 37, training loss: 41.964698791503906, validation loss: 24.151487350463867\n",
      "epoch: 38, training loss: 41.9058723449707, validation loss: 24.102075576782227\n",
      "epoch: 39, training loss: 41.847618103027344, validation loss: 24.053314208984375\n",
      "epoch: 40, training loss: 41.78994369506836, validation loss: 24.00518798828125\n",
      "epoch: 41, training loss: 41.73284912109375, validation loss: 23.957674026489258\n",
      "epoch: 42, training loss: 41.67631530761719, validation loss: 23.9107666015625\n",
      "epoch: 43, training loss: 41.62034606933594, validation loss: 23.864437103271484\n",
      "epoch: 44, training loss: 41.564945220947266, validation loss: 23.818683624267578\n",
      "epoch: 45, training loss: 41.510093688964844, validation loss: 23.773483276367188\n",
      "epoch: 46, training loss: 41.455787658691406, validation loss: 23.728830337524414\n",
      "epoch: 47, training loss: 41.40202713012695, validation loss: 23.684707641601562\n",
      "epoch: 48, training loss: 41.34879684448242, validation loss: 23.641103744506836\n",
      "epoch: 49, training loss: 41.29610061645508, validation loss: 23.598011016845703\n",
      "epoch: 50, training loss: 41.24391555786133, validation loss: 23.5554141998291\n",
      "epoch: 51, training loss: 41.19225311279297, validation loss: 23.513307571411133\n",
      "epoch: 52, training loss: 41.1411018371582, validation loss: 23.4716796875\n",
      "epoch: 53, training loss: 41.090450286865234, validation loss: 23.430519104003906\n",
      "epoch: 54, training loss: 41.04029846191406, validation loss: 23.389812469482422\n",
      "epoch: 55, training loss: 40.990631103515625, validation loss: 23.34956169128418\n",
      "epoch: 56, training loss: 40.94144058227539, validation loss: 23.309749603271484\n",
      "epoch: 57, training loss: 40.89272689819336, validation loss: 23.27037239074707\n",
      "epoch: 58, training loss: 40.844478607177734, validation loss: 23.231414794921875\n",
      "epoch: 59, training loss: 40.796695709228516, validation loss: 23.192880630493164\n",
      "epoch: 60, training loss: 40.749366760253906, validation loss: 23.154754638671875\n",
      "epoch: 61, training loss: 40.70248031616211, validation loss: 23.117029190063477\n",
      "epoch: 62, training loss: 40.656036376953125, validation loss: 23.079700469970703\n",
      "epoch: 63, training loss: 40.61003494262695, validation loss: 23.042757034301758\n",
      "epoch: 64, training loss: 40.564456939697266, validation loss: 23.006200790405273\n",
      "epoch: 65, training loss: 40.5192985534668, validation loss: 22.970006942749023\n",
      "epoch: 66, training loss: 40.47456359863281, validation loss: 22.93419075012207\n",
      "epoch: 67, training loss: 40.430233001708984, validation loss: 22.89873695373535\n",
      "epoch: 68, training loss: 40.38631057739258, validation loss: 22.86363410949707\n",
      "epoch: 69, training loss: 40.3427848815918, validation loss: 22.82888412475586\n",
      "epoch: 70, training loss: 40.29964828491211, validation loss: 22.794475555419922\n",
      "epoch: 71, training loss: 40.25690460205078, validation loss: 22.760406494140625\n",
      "epoch: 72, training loss: 40.214542388916016, validation loss: 22.726673126220703\n",
      "epoch: 73, training loss: 40.17255401611328, validation loss: 22.69326400756836\n",
      "epoch: 74, training loss: 40.13093948364258, validation loss: 22.660173416137695\n",
      "epoch: 75, training loss: 40.08968734741211, validation loss: 22.627405166625977\n",
      "epoch: 76, training loss: 40.048797607421875, validation loss: 22.594945907592773\n",
      "epoch: 77, training loss: 40.00825881958008, validation loss: 22.562793731689453\n",
      "epoch: 78, training loss: 39.968074798583984, validation loss: 22.530941009521484\n",
      "epoch: 79, training loss: 39.9282341003418, validation loss: 22.499391555786133\n",
      "epoch: 80, training loss: 39.88873291015625, validation loss: 22.468128204345703\n",
      "epoch: 81, training loss: 39.84957504272461, validation loss: 22.437156677246094\n",
      "epoch: 82, training loss: 39.81074523925781, validation loss: 22.406469345092773\n",
      "epoch: 83, training loss: 39.77223587036133, validation loss: 22.37605857849121\n",
      "epoch: 84, training loss: 39.734046936035156, validation loss: 22.34592628479004\n",
      "epoch: 85, training loss: 39.69618606567383, validation loss: 22.316062927246094\n",
      "epoch: 86, training loss: 39.65863037109375, validation loss: 22.286466598510742\n",
      "epoch: 87, training loss: 39.62138748168945, validation loss: 22.257129669189453\n",
      "epoch: 88, training loss: 39.58444595336914, validation loss: 22.228057861328125\n",
      "epoch: 89, training loss: 39.54780578613281, validation loss: 22.19923973083496\n",
      "epoch: 90, training loss: 39.51146697998047, validation loss: 22.170669555664062\n",
      "epoch: 91, training loss: 39.47541809082031, validation loss: 22.14234733581543\n",
      "epoch: 92, training loss: 39.43965530395508, validation loss: 22.11427116394043\n",
      "epoch: 93, training loss: 39.4041748046875, validation loss: 22.086435317993164\n",
      "epoch: 94, training loss: 39.368988037109375, validation loss: 22.058834075927734\n",
      "epoch: 95, training loss: 39.334068298339844, validation loss: 22.031469345092773\n",
      "epoch: 96, training loss: 39.29942321777344, validation loss: 22.004335403442383\n",
      "epoch: 97, training loss: 39.26504898071289, validation loss: 21.977428436279297\n",
      "epoch: 98, training loss: 39.23094177246094, validation loss: 21.95074462890625\n",
      "epoch: 99, training loss: 39.19709777832031, validation loss: 21.92428207397461\n",
      "epoch: 100, training loss: 39.163516998291016, validation loss: 21.898038864135742\n",
      "Predictions for miami generated!\n",
      "Done for miami\n",
      "epoch: 1, training loss: 49.60157012939453, validation loss: 42.8697395324707\n",
      "epoch: 2, training loss: 49.38865661621094, validation loss: 43.256134033203125\n",
      "epoch: 3, training loss: 49.21280288696289, validation loss: 43.587345123291016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, training loss: 49.06246566772461, validation loss: 43.87137985229492\n",
      "epoch: 5, training loss: 48.93068313598633, validation loss: 44.11536407470703\n",
      "epoch: 6, training loss: 48.8128547668457, validation loss: 44.32532501220703\n",
      "epoch: 7, training loss: 48.7057991027832, validation loss: 44.506473541259766\n",
      "epoch: 8, training loss: 48.60726547241211, validation loss: 44.66313171386719\n",
      "epoch: 9, training loss: 48.51560592651367, validation loss: 44.79899597167969\n",
      "epoch: 10, training loss: 48.42955780029297, validation loss: 44.91717529296875\n",
      "epoch: 11, training loss: 48.34819793701172, validation loss: 45.02030563354492\n",
      "epoch: 12, training loss: 48.27081298828125, validation loss: 45.1106071472168\n",
      "epoch: 13, training loss: 48.196800231933594, validation loss: 45.18996047973633\n",
      "epoch: 14, training loss: 48.12571716308594, validation loss: 45.25996398925781\n",
      "epoch: 15, training loss: 48.05718994140625, validation loss: 45.321964263916016\n",
      "epoch: 16, training loss: 47.99092102050781, validation loss: 45.37711715698242\n",
      "epoch: 17, training loss: 47.926639556884766, validation loss: 45.42637634277344\n",
      "epoch: 18, training loss: 47.8641471862793, validation loss: 45.47057342529297\n",
      "epoch: 19, training loss: 47.803245544433594, validation loss: 45.51042938232422\n",
      "epoch: 20, training loss: 47.743797302246094, validation loss: 45.546539306640625\n",
      "epoch: 21, training loss: 47.6856575012207, validation loss: 45.579410552978516\n",
      "epoch: 22, training loss: 47.628726959228516, validation loss: 45.60948944091797\n",
      "epoch: 23, training loss: 47.572872161865234, validation loss: 45.63714599609375\n",
      "epoch: 24, training loss: 47.518035888671875, validation loss: 45.66268539428711\n",
      "epoch: 25, training loss: 47.46413040161133, validation loss: 45.6864128112793\n",
      "epoch: 26, training loss: 47.411075592041016, validation loss: 45.708560943603516\n",
      "epoch: 27, training loss: 47.358821868896484, validation loss: 45.72934341430664\n",
      "epoch: 28, training loss: 47.30730438232422, validation loss: 45.7489128112793\n",
      "epoch: 29, training loss: 47.25647735595703, validation loss: 45.76746368408203\n",
      "epoch: 30, training loss: 47.2063102722168, validation loss: 45.78507995605469\n",
      "epoch: 31, training loss: 47.1567497253418, validation loss: 45.801902770996094\n",
      "epoch: 32, training loss: 47.107757568359375, validation loss: 45.81803512573242\n",
      "epoch: 33, training loss: 47.05931854248047, validation loss: 45.833560943603516\n",
      "epoch: 34, training loss: 47.01138687133789, validation loss: 45.84854507446289\n",
      "epoch: 35, training loss: 46.96395492553711, validation loss: 45.863059997558594\n",
      "epoch: 36, training loss: 46.91697692871094, validation loss: 45.87717819213867\n",
      "epoch: 37, training loss: 46.870445251464844, validation loss: 45.89093017578125\n",
      "epoch: 38, training loss: 46.824344635009766, validation loss: 45.90436935424805\n",
      "epoch: 39, training loss: 46.778648376464844, validation loss: 45.917545318603516\n",
      "epoch: 40, training loss: 46.73333740234375, validation loss: 45.930477142333984\n",
      "epoch: 41, training loss: 46.68840789794922, validation loss: 45.943199157714844\n",
      "epoch: 42, training loss: 46.64383316040039, validation loss: 45.95574951171875\n",
      "epoch: 43, training loss: 46.599613189697266, validation loss: 45.96814727783203\n",
      "epoch: 44, training loss: 46.55573272705078, validation loss: 45.98039245605469\n",
      "epoch: 45, training loss: 46.51218032836914, validation loss: 45.992515563964844\n",
      "epoch: 46, training loss: 46.46894836425781, validation loss: 46.00455856323242\n",
      "epoch: 47, training loss: 46.42601013183594, validation loss: 46.016510009765625\n",
      "epoch: 48, training loss: 46.383384704589844, validation loss: 46.028385162353516\n",
      "epoch: 49, training loss: 46.341060638427734, validation loss: 46.040199279785156\n",
      "epoch: 50, training loss: 46.299015045166016, validation loss: 46.051971435546875\n",
      "epoch: 51, training loss: 46.25725173950195, validation loss: 46.06368637084961\n",
      "epoch: 52, training loss: 46.21575927734375, validation loss: 46.075382232666016\n",
      "epoch: 53, training loss: 46.17452621459961, validation loss: 46.08705520629883\n",
      "epoch: 54, training loss: 46.13357162475586, validation loss: 46.09870529174805\n",
      "epoch: 55, training loss: 46.09286117553711, validation loss: 46.110347747802734\n",
      "epoch: 56, training loss: 46.05241394042969, validation loss: 46.121978759765625\n",
      "epoch: 57, training loss: 46.01220703125, validation loss: 46.133609771728516\n",
      "epoch: 58, training loss: 45.97224426269531, validation loss: 46.14523696899414\n",
      "epoch: 59, training loss: 45.932525634765625, validation loss: 46.15687561035156\n",
      "epoch: 60, training loss: 45.89303207397461, validation loss: 46.168521881103516\n",
      "epoch: 61, training loss: 45.85377883911133, validation loss: 46.18019104003906\n",
      "epoch: 62, training loss: 45.814754486083984, validation loss: 46.191864013671875\n",
      "epoch: 63, training loss: 45.77594757080078, validation loss: 46.20356750488281\n",
      "epoch: 64, training loss: 45.737369537353516, validation loss: 46.215301513671875\n",
      "epoch: 65, training loss: 45.69900131225586, validation loss: 46.227046966552734\n",
      "epoch: 66, training loss: 45.66085433959961, validation loss: 46.23883056640625\n",
      "epoch: 67, training loss: 45.6229248046875, validation loss: 46.25062561035156\n",
      "epoch: 68, training loss: 45.585205078125, validation loss: 46.262454986572266\n",
      "epoch: 69, training loss: 45.54768371582031, validation loss: 46.274322509765625\n",
      "epoch: 70, training loss: 45.510372161865234, validation loss: 46.286216735839844\n",
      "epoch: 71, training loss: 45.47325897216797, validation loss: 46.29813766479492\n",
      "epoch: 72, training loss: 45.43635559082031, validation loss: 46.31010055541992\n",
      "epoch: 73, training loss: 45.3996467590332, validation loss: 46.32210922241211\n",
      "epoch: 74, training loss: 45.36313247680664, validation loss: 46.33414077758789\n",
      "epoch: 75, training loss: 45.32680892944336, validation loss: 46.34621047973633\n",
      "epoch: 76, training loss: 45.290679931640625, validation loss: 46.35832977294922\n",
      "epoch: 77, training loss: 45.25474166870117, validation loss: 46.37046813964844\n",
      "epoch: 78, training loss: 45.218990325927734, validation loss: 46.38265609741211\n",
      "epoch: 79, training loss: 45.18342208862305, validation loss: 46.394893646240234\n",
      "epoch: 80, training loss: 45.14804458618164, validation loss: 46.40715789794922\n",
      "epoch: 81, training loss: 45.11284637451172, validation loss: 46.41945266723633\n",
      "epoch: 82, training loss: 45.077823638916016, validation loss: 46.43179702758789\n",
      "epoch: 83, training loss: 45.04298400878906, validation loss: 46.444175720214844\n",
      "epoch: 84, training loss: 45.00831604003906, validation loss: 46.45659637451172\n",
      "epoch: 85, training loss: 44.97382736206055, validation loss: 46.469078063964844\n",
      "epoch: 86, training loss: 44.93951416015625, validation loss: 46.48157501220703\n",
      "epoch: 87, training loss: 44.905364990234375, validation loss: 46.4941291809082\n",
      "epoch: 88, training loss: 44.87138366699219, validation loss: 46.50672149658203\n",
      "epoch: 89, training loss: 44.83757781982422, validation loss: 46.519351959228516\n",
      "epoch: 90, training loss: 44.80393981933594, validation loss: 46.532012939453125\n",
      "epoch: 91, training loss: 44.77046203613281, validation loss: 46.54471969604492\n",
      "epoch: 92, training loss: 44.73715591430664, validation loss: 46.55746841430664\n",
      "epoch: 93, training loss: 44.704010009765625, validation loss: 46.57026672363281\n",
      "epoch: 94, training loss: 44.671024322509766, validation loss: 46.58308792114258\n",
      "epoch: 95, training loss: 44.63819122314453, validation loss: 46.595970153808594\n",
      "epoch: 96, training loss: 44.605525970458984, validation loss: 46.6088752746582\n",
      "epoch: 97, training loss: 44.57301330566406, validation loss: 46.62181854248047\n",
      "epoch: 98, training loss: 44.54066467285156, validation loss: 46.63481140136719\n",
      "epoch: 99, training loss: 44.508460998535156, validation loss: 46.64783477783203\n",
      "epoch: 100, training loss: 44.47641372680664, validation loss: 46.66090393066406\n",
      "Predictions for pittsburgh generated!\n",
      "Done for pittsburgh\n",
      "epoch: 1, training loss: 35.49617004394531, validation loss: 21.159778594970703\n",
      "epoch: 2, training loss: 35.38358688354492, validation loss: 21.203014373779297\n",
      "epoch: 3, training loss: 35.285587310791016, validation loss: 21.248050689697266\n",
      "epoch: 4, training loss: 35.19966125488281, validation loss: 21.29300880432129\n",
      "epoch: 5, training loss: 35.123783111572266, validation loss: 21.337116241455078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, training loss: 35.05636215209961, validation loss: 21.379932403564453\n",
      "epoch: 7, training loss: 34.996116638183594, validation loss: 21.42118263244629\n",
      "epoch: 8, training loss: 34.94198989868164, validation loss: 21.46070098876953\n",
      "epoch: 9, training loss: 34.893131256103516, validation loss: 21.498374938964844\n",
      "epoch: 10, training loss: 34.84881591796875, validation loss: 21.534151077270508\n",
      "epoch: 11, training loss: 34.80845260620117, validation loss: 21.568008422851562\n",
      "epoch: 12, training loss: 34.77152633666992, validation loss: 21.599950790405273\n",
      "epoch: 13, training loss: 34.737606048583984, validation loss: 21.630001068115234\n",
      "epoch: 14, training loss: 34.706329345703125, validation loss: 21.658203125\n",
      "epoch: 15, training loss: 34.6773681640625, validation loss: 21.684595108032227\n",
      "epoch: 16, training loss: 34.65047073364258, validation loss: 21.70924186706543\n",
      "epoch: 17, training loss: 34.62538146972656, validation loss: 21.732196807861328\n",
      "epoch: 18, training loss: 34.60190963745117, validation loss: 21.753528594970703\n",
      "epoch: 19, training loss: 34.579872131347656, validation loss: 21.773298263549805\n",
      "epoch: 20, training loss: 34.559112548828125, validation loss: 21.79157829284668\n",
      "epoch: 21, training loss: 34.53950500488281, validation loss: 21.80843162536621\n",
      "epoch: 22, training loss: 34.52091598510742, validation loss: 21.823928833007812\n",
      "epoch: 23, training loss: 34.50324630737305, validation loss: 21.838130950927734\n",
      "epoch: 24, training loss: 34.48640823364258, validation loss: 21.851102828979492\n",
      "epoch: 25, training loss: 34.4703254699707, validation loss: 21.8629093170166\n",
      "epoch: 26, training loss: 34.454917907714844, validation loss: 21.87360954284668\n",
      "epoch: 27, training loss: 34.44012451171875, validation loss: 21.88326072692871\n",
      "epoch: 28, training loss: 34.42588806152344, validation loss: 21.891918182373047\n",
      "epoch: 29, training loss: 34.41215896606445, validation loss: 21.899639129638672\n",
      "epoch: 30, training loss: 34.39889907836914, validation loss: 21.906469345092773\n",
      "epoch: 31, training loss: 34.38606262207031, validation loss: 21.912464141845703\n",
      "epoch: 32, training loss: 34.37361526489258, validation loss: 21.91766357421875\n",
      "epoch: 33, training loss: 34.36153030395508, validation loss: 21.922117233276367\n",
      "epoch: 34, training loss: 34.34977722167969, validation loss: 21.925867080688477\n",
      "epoch: 35, training loss: 34.338321685791016, validation loss: 21.928953170776367\n",
      "epoch: 36, training loss: 34.327152252197266, validation loss: 21.931411743164062\n",
      "epoch: 37, training loss: 34.31624221801758, validation loss: 21.933277130126953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_75/2557582484.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mt_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    108\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_sz = 64\n",
    "\n",
    "header = ['ID'] + ['v' + str(i) for i in range(0, 120)] \n",
    "with open('./lr_pytorch_submission.csv', 'w') as f:\n",
    "    f.write(','.join(header) + '\\n')\n",
    "    \n",
    "    \n",
    "lr = nn.Sequential(nn.Linear(100,120)).to(device)\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lr.parameters(), lr=0.001)    \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.9)\n",
    "\n",
    "for city in cities:\n",
    "    #lr = nn.Sequential(nn.Linear(100,120)).to(device)\n",
    "    #loss = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(lr.parameters(), lr=0.0001)    \n",
    "    \n",
    "    \n",
    "    training_data = ArgoverseDataset(city=city, split='train')\n",
    "    train_loader = DataLoader(training_data, batch_size=batch_sz)\n",
    "    val_data = ArgoverseDataset(city=city, split='val')\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_sz)\n",
    "    test_data = ArgoverseDataset(city=city, split='test')\n",
    "    test_loader = DataLoader(test_data, batch_size=1)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    num_epochs=100\n",
    "    for epoch in range(num_epochs):\n",
    "        for i_batch, batch in enumerate(train_loader):\n",
    "            inp, out = batch\n",
    "            inp = inp.float().to(device)\n",
    "            out = out.float().to(device)\n",
    "            inp = inp.view(inp.shape[0] ,inp.shape[1] * 2)\n",
    "            pred = lr(inp)\n",
    "            pred = pred.reshape(pred.shape[0], 60, 2)\n",
    "            t_l = loss(pred, out)\n",
    "            optimizer.zero_grad()\n",
    "            t_l.backward()\n",
    "            optimizer.step()\n",
    "        train_losses.append(t_l.item())\n",
    "        for i_batch, batch in enumerate(val_loader):\n",
    "                with torch.no_grad():\n",
    "                    inp, out = batch\n",
    "                    inp = inp.float().to(device)\n",
    "                    out = out.float().to(device)\n",
    "                    inp = inp.view(inp.shape[0] ,inp.shape[1] * 2)\n",
    "                    \n",
    "                    pred = lr(inp)\n",
    "                    pred = pred.reshape(pred.shape[0], 60, 2)\n",
    "\n",
    "                    v_l = loss(pred, out)\n",
    "        print('epoch: {}, training loss: {}, validation loss: {}'.format(epoch + 1, t_l, v_l))\n",
    "        scheduler.step()     \n",
    "    \n",
    "    scene = 0\n",
    "    output = ''\n",
    "    with torch.no_grad():\n",
    "        for i_batch, batch in enumerate(test_loader):\n",
    "            inp = batch\n",
    "            inp = inp.float().to(device)\n",
    "            inp = inp.view(inp.shape[0] ,inp.shape[1] * 2)\n",
    "            \n",
    "            preds = lr(inp)\n",
    "            preds = preds.reshape(preds.shape[0], 60, 2)\n",
    "            flat = preds[0].flatten().cpu().tolist()\n",
    "            \n",
    "            row = ['{}_{}'.format(scene, city)] + flat\n",
    "            row = [str(i) for i in row]\n",
    "            output += ','.join(row) + '\\n'\n",
    "            \n",
    "            scene += 1\n",
    "    \n",
    "    try:\n",
    "        with open('./lr_pytorch_submission.csv', 'a') as f:\n",
    "            f.write(output)\n",
    "        print('Predictions for {} generated!'.format(city))\n",
    "    except:\n",
    "        print('Error! Unsuccessful write...')\n",
    "        \n",
    "    print('Done for {}'.format(city))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a599cd70-2624-48eb-8659-250defafd740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
