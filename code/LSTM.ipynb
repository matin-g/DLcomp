{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", valid=False, normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "        \n",
    "        if valid:\n",
    "            idx = int(len(inputs) * .8)\n",
    "            return inputs[:idx], inputs[idx:], outputs[:idx], outputs[idx:]\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            data = (self.inputs[idx], self.outputs[idx])\n",
    "        else:\n",
    "            data = (self.inputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "    \n",
    "class ValidationDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, inputs, outputs, transform=None):\n",
    "        super(ValidationDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.inputs, self.outputs = inputs, outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'palo-alto' \n",
    "split = 'train'\n",
    "train_dataset  = ArgoverseDataset(city = city, split = split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 32  # batch size \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import random\n",
    "\n",
    "# def show_sample_batch(sample_batch):\n",
    "#     \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "#     inp, out = sample_batch\n",
    "#     batch_sz = inp.size(0)\n",
    "#     agent_sz = inp.size(1)\n",
    "    \n",
    "#     fig, axs = plt.subplots(1, batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "#     fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "#     axs = axs.ravel()   \n",
    "#     for i in range(batch_sz):\n",
    "#         axs[i].xaxis.set_ticks([])\n",
    "#         axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "#         axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "#         axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "\n",
    "        \n",
    "# for i_batch, sample_batch in enumerate(train_loader):\n",
    "#     # inp[i] is a scene with 50 coordinates, input[i, j] is a coordinate\n",
    "#     # gotta loop through each scene in the batch\n",
    "#     inp, out = sample_batch # inp: (batch size, 50, 2), out: (batch size, 60, 2)\n",
    "#     \"\"\"\n",
    "#     TODO:\n",
    "#       implement your Deep learning model\n",
    "#       implement training routine\n",
    "#     \"\"\"\n",
    "#     show_sample_batch(sample_batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2222c",
   "metadata": {},
   "source": [
    "## These models were trained with SGD as the opto and it wasn't very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "582919b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(2, 256, 2).to(device)\n",
    "# opto = torch.optim.SGD(model.parameters(), lr=1)\n",
    "# loss_fct = nn.MSELoss()\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a200af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(2, 128, 2).to(device)\n",
    "# opto = torch.optim.SGD(model.parameters(), lr=1)\n",
    "# loss_fct = nn.MSELoss()\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea3aa6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(2, 128, 2).to(device)\n",
    "# opto = torch.optim.SGD(model.parameters(), lr=.1)\n",
    "# loss_fct = nn.MSELoss()\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f7c35",
   "metadata": {},
   "source": [
    "## Started using Adam optimizer / lstm with dropout, these next 2 models are usable, adding l2 loss didnt converge, currently using the second one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ceae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(input_size=2, hidden_size=128, output_size=2, num_layers=2, dropout=.2).to(device)\n",
    "# opto = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "# loss_fct = nn.MSELoss()\n",
    "\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d4aad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(input_size=2, hidden_size=256, output_size=2, num_layers=2, dropout=.2).to(device)\n",
    "# opto = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "# loss_fct = nn.MSELoss()\n",
    "\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183cd4f",
   "metadata": {},
   "source": [
    "## Bidirectional doesn't make sense here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6cf9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(2, 256, 2, .2, True).to(device)\n",
    "# opto = torch.optim.Adam(model.parameters(), lr=.001, weight_decay=.01)\n",
    "# loss_fct = nn.MSELoss()\n",
    "\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a9ca7",
   "metadata": {},
   "source": [
    "# Test Dataset and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1678689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(fp, model, opto, loss_fct, device, valid=False):\n",
    "    cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"] \n",
    "    header = ['ID'] + ['v' + str(i) for i in range(0, 120)]\n",
    "    \n",
    "    with open(fp, 'w') as f:\n",
    "        f.write(','.join(header) + '\\n')\n",
    "\n",
    "    for city in cities:\n",
    "        batch_sz = 256\n",
    "        if valid:\n",
    "            i, v_i, o, v_o = get_city_trajectories(city=city, split=\"train\", valid=valid)\n",
    "            training_data = ValidationDataset(i, o)\n",
    "            validation_data = ValidationDataset(v_i, v_o)\n",
    "            train_loader = DataLoader(training_data, batch_size=batch_sz)\n",
    "            val_loader = DataLoader(validation_data, batch_size=batch_sz)\n",
    "        else:\n",
    "            val_loader = None\n",
    "            training_data = ArgoverseDataset(city=city, split='train')\n",
    "            train_loader = DataLoader(training_data, batch_size=batch_sz, drop_last=True)\n",
    "        \n",
    "        train(model, 100, train_loader, loss_fct, opto, device, val_loader, valid)\n",
    "        \n",
    "        if not valid:\n",
    "            test_dataset  = ArgoverseDataset(city=city, split='test')\n",
    "            test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "            write_city_preds(model, test_loader, device, city, fp)\n",
    "\n",
    "    print(fp + ' generated!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a31241bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler():\n",
    "    \"\"\"Standardize data by removing the mean and scaling to\n",
    "    unit variance.  This object can be used as a transform\n",
    "    in PyTorch data loaders.\n",
    "\n",
    "    Args:\n",
    "        mean (FloatTensor): The mean value for each feature in the data.\n",
    "        scale (FloatTensor): Per-feature relative scaling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean=None, scale=None):\n",
    "        if mean is not None:\n",
    "            mean = torch.FloatTensor(mean)\n",
    "        if scale is not None:\n",
    "            scale = torch.FloatTensor(scale)\n",
    "        self.mean_ = mean\n",
    "        self.scale_ = scale\n",
    "\n",
    "    def fit(self, sample):\n",
    "        \"\"\"Set the mean and scale values based on the sample data.\n",
    "        \"\"\"\n",
    "        self.mean_ = sample.mean(0, keepdim=True)\n",
    "        self.scale_ = sample.std(0, unbiased=False, keepdim=True)\n",
    "        return self\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return (sample - self.mean_)/self.scale_\n",
    "\n",
    "    def inverse_transform(self, sample):\n",
    "        \"\"\"Scale the data back to the original representation\n",
    "        \"\"\"\n",
    "        #print(sample.size())\n",
    "        #print(self.scale_.size())\n",
    "        #print(self.mean_)\n",
    "        return sample * self.scale_ + self.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fe4eb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from d2l.ai\n",
    "def grad_clipping(net, theta):\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device, num_layers=2, dropout=0, bidirectional=False):\n",
    "        super(RNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, dropout=dropout, bidirectional=False, batch_first=True)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "        else:\n",
    "            #self.act1 = nn.ReLU(inplace = True)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "            \n",
    "        #self.act1 = nn.ReLU(inplace = True)\n",
    "        #self.bn1 = nn.BatchNorm1d(60)\n",
    "        \n",
    "        #self.estimator = nn.Linear(60, 2)\n",
    "\n",
    "            \n",
    "        #self.batch_normalisation1 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        #x = x.to(self.device)\n",
    "        \n",
    "        x = F.layer_norm(x, x.size())\n",
    "        #x = self.batch_normalisation1(x)\n",
    "        # 32, 50, 2\n",
    "        # 32, 1, 2\n",
    "        for i in range(50):\n",
    "            #print(i)\n",
    "            #print(h[0])\n",
    "            out, (h,c) = self.rnn(x[:,i,:].unsqueeze(1), (h.detach(),c.detach()))\n",
    "        \n",
    "        #out_all = torch.tensor(())\n",
    "        #out_all = out_all.to(self.device)\n",
    "        out_all = []\n",
    "        for i in range(60):\n",
    "            \n",
    "            v, (h,c) = self.rnn(x[:,-1:,:], (h.detach(),c.detach()))\n",
    "            #print(h.shape)\n",
    "            #print(h.size())\n",
    "            #out = self.act1(h)\n",
    "            out = self.fc(h)\n",
    "#             print('hi')\n",
    "            #print(out.size())\n",
    "            #outputs.append(out)\n",
    "            \n",
    "            out_all.append(out)\n",
    "        out_all = torch.cat(out_all, dim = 0)\n",
    "            #out_all = torch.cat((out_all, out))\n",
    "        \n",
    "        #print(out_all.size())\n",
    "        #out_all.reshape(32,-1,2)\n",
    "        #out=out_all.transpose(0,1)\n",
    "#         out = self.act1(self.bn1(out))\n",
    "#         out = self.estimator(out)\n",
    "            \n",
    "        return out_all.transpose(0,1), (h.detach(),c.detach())#_all.reshape(-1, 60, 2), (h, c) #.reshape(-1, 60, 2)\n",
    "        \n",
    "#         out, _ = self.rnn(x)\n",
    "        \n",
    "#         out = self.fc(out)\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        #print(out_all.reshape(32,-1,2).size())\n",
    "#         print(torch.cat(outputs, dim=1))\n",
    "#out_all = out_all.to(self.device)\n",
    "        #print(out.size())\n",
    "        #out = self.fc(out)\n",
    "       \n",
    "    \n",
    "    def init_hidden(self, batch_size=512):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, cell\n",
    "\n",
    "      \n",
    "        #out, _ = self.rnn(x)\n",
    "        \n",
    "        #out = self.fc(out)\n",
    "\n",
    "        #return out\n",
    "\n",
    "import seaborn as sns\n",
    "def train(net, n_epochs, train_loader, loss_fct, criterion, device, val_loader=None, valid=False):\n",
    "    train_l= []\n",
    "    val_l = []\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for epoch in range(n_epochs):\n",
    "        ## training loop\n",
    "        hidden, cell = net.init_hidden(256)\n",
    "        t0 = time.time()\n",
    "        for i_batch, batch in enumerate(train_loader):\n",
    "            #print(len([x[0,0] for x, y in train_loader]))\n",
    "            #print(len([x[0,0] for x, y in train_loader]))\n",
    "            #criterion.zero_grad(set_to_none=True)\n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "            # inp[i] is a scene with 50 coordinates, input[i, j] is a coordinate\n",
    "            inp, out = batch\n",
    "            inp = inp.float().to(device)\n",
    "            out = out.float().to(device)\n",
    "            \n",
    "                        \n",
    "            foo = StandardScaler()\n",
    "            foo.fit(inp)\n",
    "            #print(inp)\n",
    "            #print(foo(inp))\n",
    "#             print(foo.fit(inp))\n",
    "#             print(StandardScaler())\n",
    "            \n",
    "            # 0 pad the end of input seq\n",
    "            #inp = torch.cat((inp, torch.zeros(inp.size(0), 10, inp.size(2), device=device)), dim=1)\n",
    "            \n",
    "            #print(inp.size())\n",
    "    \n",
    "            #inp = torch.cat((inp, torch.zeros(inp.size(0), 10, inp.size(2), device=device)), dim=1)\n",
    "            #pred = net(inp).to(device)\n",
    "            #.to(device)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred, (h,c) = net(foo(inp), hidden, cell)\n",
    "            # print('input: {}'.format(inp[0, :3]))\n",
    "            # print('preds: {}'.format(pred[0, :3]))\n",
    "            # print('true: {}'.format(out[0, :3]))\n",
    "#             print('*****************************')      \n",
    "#             print(pred.size())\n",
    "#             #print(pred)\n",
    "#             print('*****************************')\n",
    "#             print(out.size())\n",
    "            #print(out)\n",
    "                #print(pred)\n",
    "                #inp2 = inp\n",
    "                #inp2 = torch.cat((inp, torch.zeros(inp2.size(inp2.mean(0, keepdim=True)), 10, inp.size(2), device=device)), dim=1)\n",
    "                foo2 = StandardScaler()\n",
    "                foo2.fit(out)\n",
    "                #print(pred.size())\n",
    "                #print(pred[:, None, None].size())\n",
    "                \n",
    "                loss = loss_fct(foo2.inverse_transform(pred), out)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(criterion)\n",
    "            grad_clipping(net, 1)\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=0.1, norm_type=\"inf\")\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
    "            #torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "            scaler.step(criterion)\n",
    "            scaler.update()\n",
    "\n",
    "            #print(loss.item())\n",
    "#             print(len(batch))\n",
    "            #print(i_batch)\n",
    "\n",
    "#             criterion.zero_grad()\n",
    "#             #grad_clipping(net, 1)\n",
    "#             loss.backward()\n",
    "#             grad_clipping(net, 1)\n",
    "#             criterion.step()\n",
    "        #print('bi')\n",
    "        print('{} seconds'.format(time.time() - t0))\n",
    "            \n",
    "        train_l.append(loss.item())\n",
    "        \n",
    "        if valid:\n",
    "            for i_batch, batch in enumerate(val_loader):\n",
    "                with torch.no_grad():\n",
    "                    inp, out = batch\n",
    "                    inp = inp.float().to(device)\n",
    "                    out = out.float().to(device)\n",
    "\n",
    "                    inp = torch.cat((inp, torch.zeros(inp.size(0), 10, inp.size(2), device=device)), dim=1)\n",
    "                    pred = net(inp).to(device)\n",
    "\n",
    "                    val_loss = loss_fct(pred, out)\n",
    "        if valid:\n",
    "            val_l.append(val_loss.item())\n",
    "            \n",
    "            print('epoch: {}, training loss: {}, validation loss: {}'.format(epoch + 1, loss, val_loss))\n",
    "        else:\n",
    "            print('epoch: {}, training loss: {}'.format(epoch + 1, loss))\n",
    "        \n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    sns.lineplot(ax=ax[0], x=np.arange(0, len(train_l)), y=train_l)\n",
    "    if valid:\n",
    "        sns.lineplot(ax=ax[1], x=np.arange(0, len(val_l)), y=val_l)\n",
    "    print('-'* 70)\n",
    "    return\n",
    "\n",
    "# def predict(net, data_loader, device):\n",
    "#     with torch.no_grad():\n",
    "#         for i_batch, batch in enumerate(data_loader):\n",
    "#             inp, _ = batch\n",
    "#             inp = torch.cat((inp.float().to(device), torch.zeros(inp.size(0), 10, inp.size(2)).to(device)), dim=1)\n",
    "                        \n",
    "#             preds = net(inp.float().to(device))\n",
    "            \n",
    "#             return preds\n",
    "        \n",
    "def write_city_preds(net, test_loader, device, city, fp):       \n",
    "    scene = 0\n",
    "    output = ''\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i_batch, batch in enumerate(test_loader):\n",
    "            inp = batch\n",
    "            inp = inp.float().to(device)\n",
    "            inp = torch.cat((inp, torch.zeros(inp.size(0), 10, inp.size(2), device=device)), dim=1)\n",
    "\n",
    "            preds = net(inp)\n",
    "            flat = preds[0].flatten().cpu().tolist()\n",
    "            \n",
    "            row = ['{}_{}'.format(scene, city)] + flat\n",
    "            row = [str(i) for i in row]\n",
    "            output += ','.join(row) + '\\n'\n",
    "            \n",
    "            scene += 1\n",
    "    \n",
    "    try:\n",
    "        with open('./submission.csv', 'a') as f:\n",
    "            f.write(output)\n",
    "        print('Predictions for {} generated!'.format(city))\n",
    "        return 1\n",
    "    except:\n",
    "        print('Error! Unsuccessful write...')\n",
    "        return -1\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6a9b7437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.32648730278015 seconds\n",
      "epoch: 1, training loss: 3702.25\n",
      "15.746092319488525 seconds\n",
      "epoch: 2, training loss: 1378.131103515625\n",
      "16.25423502922058 seconds\n",
      "epoch: 3, training loss: 2374.871826171875\n",
      "16.40245747566223 seconds\n",
      "epoch: 4, training loss: 1887.901123046875\n",
      "16.075568199157715 seconds\n",
      "epoch: 5, training loss: 2378.00634765625\n",
      "16.621626138687134 seconds\n",
      "epoch: 6, training loss: 1406.84716796875\n",
      "16.64114022254944 seconds\n",
      "epoch: 7, training loss: 3722.336669921875\n",
      "16.395253658294678 seconds\n",
      "epoch: 8, training loss: 1300.8133544921875\n",
      "16.23365354537964 seconds\n",
      "epoch: 9, training loss: 1019.3419799804688\n",
      "16.38461184501648 seconds\n",
      "epoch: 10, training loss: 3263.212158203125\n",
      "16.231130838394165 seconds\n",
      "epoch: 11, training loss: 813.7646484375\n",
      "16.51542353630066 seconds\n",
      "epoch: 12, training loss: 755.7780151367188\n",
      "16.840901374816895 seconds\n",
      "epoch: 13, training loss: 1025.8909912109375\n",
      "16.623230934143066 seconds\n",
      "epoch: 14, training loss: 611.5321655273438\n",
      "16.34329652786255 seconds\n",
      "epoch: 15, training loss: 1417.3258056640625\n",
      "15.994338035583496 seconds\n",
      "epoch: 16, training loss: 677.7601928710938\n",
      "15.857698678970337 seconds\n",
      "epoch: 17, training loss: 640.6060180664062\n",
      "16.384199142456055 seconds\n",
      "epoch: 18, training loss: 780.581298828125\n",
      "16.17534875869751 seconds\n",
      "epoch: 19, training loss: 1004.3743896484375\n",
      "16.35925555229187 seconds\n",
      "epoch: 20, training loss: 591.4920654296875\n",
      "16.332634925842285 seconds\n",
      "epoch: 21, training loss: 781.9173583984375\n",
      "16.299271821975708 seconds\n",
      "epoch: 22, training loss: 930.5267333984375\n",
      "16.146352529525757 seconds\n",
      "epoch: 23, training loss: 679.763671875\n",
      "15.677389860153198 seconds\n",
      "epoch: 24, training loss: 543.3309936523438\n",
      "15.800033330917358 seconds\n",
      "epoch: 25, training loss: 538.8141479492188\n",
      "15.816235780715942 seconds\n",
      "epoch: 26, training loss: 566.4276733398438\n",
      "15.973160743713379 seconds\n",
      "epoch: 27, training loss: 540.9496459960938\n",
      "15.866416454315186 seconds\n",
      "epoch: 28, training loss: 646.5838623046875\n",
      "16.211060285568237 seconds\n",
      "epoch: 29, training loss: 629.2199096679688\n",
      "15.685981035232544 seconds\n",
      "epoch: 30, training loss: 603.526123046875\n",
      "15.469922542572021 seconds\n",
      "epoch: 31, training loss: 726.7781982421875\n",
      "15.870378494262695 seconds\n",
      "epoch: 32, training loss: 648.4454956054688\n",
      "15.051624774932861 seconds\n",
      "epoch: 33, training loss: 636.4050903320312\n",
      "16.04131031036377 seconds\n",
      "epoch: 34, training loss: 567.5962524414062\n",
      "15.805554389953613 seconds\n",
      "epoch: 35, training loss: 680.58251953125\n",
      "15.908271074295044 seconds\n",
      "epoch: 36, training loss: 579.992919921875\n",
      "15.884312629699707 seconds\n",
      "epoch: 37, training loss: 651.1232299804688\n",
      "16.046549558639526 seconds\n",
      "epoch: 38, training loss: 593.2843017578125\n",
      "15.952410221099854 seconds\n",
      "epoch: 39, training loss: 513.1487426757812\n",
      "15.900502920150757 seconds\n",
      "epoch: 40, training loss: 709.1832275390625\n",
      "16.390843629837036 seconds\n",
      "epoch: 41, training loss: 594.117431640625\n",
      "15.729207277297974 seconds\n",
      "epoch: 42, training loss: 494.7284851074219\n",
      "15.674647092819214 seconds\n",
      "epoch: 43, training loss: 571.2161254882812\n",
      "16.01074767112732 seconds\n",
      "epoch: 44, training loss: 482.5285339355469\n",
      "15.599653482437134 seconds\n",
      "epoch: 45, training loss: 472.3336181640625\n",
      "16.072798252105713 seconds\n",
      "epoch: 46, training loss: 502.1272277832031\n",
      "16.209570169448853 seconds\n",
      "epoch: 47, training loss: 573.2171020507812\n",
      "15.927812099456787 seconds\n",
      "epoch: 48, training loss: 735.2960205078125\n",
      "16.61307668685913 seconds\n",
      "epoch: 49, training loss: 958.0787963867188\n",
      "15.959220886230469 seconds\n",
      "epoch: 50, training loss: 640.8873901367188\n",
      "16.47328782081604 seconds\n",
      "epoch: 51, training loss: 712.0979614257812\n",
      "16.31621551513672 seconds\n",
      "epoch: 52, training loss: 501.7296447753906\n",
      "16.141895532608032 seconds\n",
      "epoch: 53, training loss: 5507.927734375\n",
      "16.33908462524414 seconds\n",
      "epoch: 54, training loss: 519.8905639648438\n",
      "16.29140853881836 seconds\n",
      "epoch: 55, training loss: 530.0658569335938\n",
      "16.29048991203308 seconds\n",
      "epoch: 56, training loss: 636.7719116210938\n",
      "16.287565231323242 seconds\n",
      "epoch: 57, training loss: 552.5745849609375\n",
      "16.18951940536499 seconds\n",
      "epoch: 58, training loss: 484.53753662109375\n",
      "16.337208032608032 seconds\n",
      "epoch: 59, training loss: 498.6096496582031\n",
      "16.231801748275757 seconds\n",
      "epoch: 60, training loss: 483.60772705078125\n",
      "16.402256727218628 seconds\n",
      "epoch: 61, training loss: 510.0819396972656\n",
      "15.842034816741943 seconds\n",
      "epoch: 62, training loss: 459.2032775878906\n",
      "16.064094305038452 seconds\n",
      "epoch: 63, training loss: 475.76141357421875\n",
      "16.36656355857849 seconds\n",
      "epoch: 64, training loss: 696.6587524414062\n",
      "16.204054594039917 seconds\n",
      "epoch: 65, training loss: 938.4722290039062\n",
      "16.414270162582397 seconds\n",
      "epoch: 66, training loss: 479.0293884277344\n",
      "16.380527019500732 seconds\n",
      "epoch: 67, training loss: 689.9429931640625\n",
      "16.384971141815186 seconds\n",
      "epoch: 68, training loss: 590.2645874023438\n",
      "16.45954418182373 seconds\n",
      "epoch: 69, training loss: 629.9022216796875\n",
      "16.471256971359253 seconds\n",
      "epoch: 70, training loss: 653.4674682617188\n",
      "16.246875047683716 seconds\n",
      "epoch: 71, training loss: 665.4371337890625\n",
      "16.021362781524658 seconds\n",
      "epoch: 72, training loss: 716.3086547851562\n",
      "15.960167646408081 seconds\n",
      "epoch: 73, training loss: 696.2967529296875\n",
      "16.042101860046387 seconds\n",
      "epoch: 74, training loss: 523.0266723632812\n",
      "16.263948917388916 seconds\n",
      "epoch: 75, training loss: 577.44921875\n",
      "16.427290678024292 seconds\n",
      "epoch: 76, training loss: 652.0049438476562\n",
      "16.15291476249695 seconds\n",
      "epoch: 77, training loss: 453.39794921875\n",
      "15.940335035324097 seconds\n",
      "epoch: 78, training loss: 440.7384948730469\n",
      "15.954859733581543 seconds\n",
      "epoch: 79, training loss: 516.69189453125\n",
      "16.49979281425476 seconds\n",
      "epoch: 80, training loss: 595.0316772460938\n",
      "16.077773332595825 seconds\n",
      "epoch: 81, training loss: 508.485595703125\n",
      "16.010408639907837 seconds\n",
      "epoch: 82, training loss: 636.6715087890625\n",
      "15.921772241592407 seconds\n",
      "epoch: 83, training loss: 550.894775390625\n",
      "15.714230060577393 seconds\n",
      "epoch: 84, training loss: 525.9754638671875\n",
      "15.757556915283203 seconds\n",
      "epoch: 85, training loss: 483.2891540527344\n",
      "15.541003704071045 seconds\n",
      "epoch: 86, training loss: 605.6126098632812\n",
      "15.928437948226929 seconds\n",
      "epoch: 87, training loss: 534.486328125\n",
      "15.323609113693237 seconds\n",
      "epoch: 88, training loss: 575.3831176757812\n",
      "15.312556028366089 seconds\n",
      "epoch: 89, training loss: 901.6338500976562\n",
      "15.824938774108887 seconds\n",
      "epoch: 90, training loss: 446.6446228027344\n",
      "15.69847297668457 seconds\n",
      "epoch: 91, training loss: 488.8919677734375\n",
      "15.472566604614258 seconds\n",
      "epoch: 92, training loss: 448.9842834472656\n",
      "15.908576250076294 seconds\n",
      "epoch: 93, training loss: 450.9971923828125\n",
      "15.997342109680176 seconds\n",
      "epoch: 94, training loss: 557.7098388671875\n",
      "15.861127853393555 seconds\n",
      "epoch: 95, training loss: 471.03790283203125\n",
      "15.921464443206787 seconds\n",
      "epoch: 96, training loss: 438.1665344238281\n",
      "16.030402421951294 seconds\n",
      "epoch: 97, training loss: 518.4203491210938\n",
      "16.023654222488403 seconds\n",
      "epoch: 98, training loss: 438.4436340332031\n",
      "15.862582206726074 seconds\n",
      "epoch: 99, training loss: 477.5366516113281\n",
      "16.271228790283203 seconds\n",
      "epoch: 100, training loss: 707.4993896484375\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'h' and 'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3763/2223835105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgenerate_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3763/2838152407.py\u001b[0m in \u001b[0;36mgenerate_submission\u001b[0;34m(fp, model, opto, loss_fct, device, valid)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mwrite_city_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' generated!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3763/2449936274.py\u001b[0m in \u001b[0;36mwrite_city_preds\u001b[0;34m(net, test_loader, device, city, fp)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'h' and 'c'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(input_size=2, hidden_size=512, output_size=2, device=device, num_layers=1).to(device)\n",
    "opto = torch.optim.AdamW(model.parameters(), lr=.001)\n",
    "loss_fct = nn.MSELoss()\n",
    "\n",
    "generate_submission('./submission.csv', model, opto, loss_fct, device, valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "362cf69f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.276566743850708 seconds\n",
      "epoch: 1, training loss: 1408212.0\n",
      "15.866853475570679 seconds\n",
      "epoch: 2, training loss: 889669.75\n",
      "16.89917755126953 seconds\n",
      "epoch: 3, training loss: 322283.6875\n",
      "16.006129264831543 seconds\n",
      "epoch: 4, training loss: 158296.78125\n",
      "16.661400079727173 seconds\n",
      "epoch: 5, training loss: 201401.046875\n",
      "17.04783844947815 seconds\n",
      "epoch: 6, training loss: 259916.875\n",
      "16.996500968933105 seconds\n",
      "epoch: 7, training loss: 128386.7734375\n",
      "17.318639039993286 seconds\n",
      "epoch: 8, training loss: 159494.640625\n",
      "17.103318691253662 seconds\n",
      "epoch: 9, training loss: 163016.921875\n",
      "16.82403016090393 seconds\n",
      "epoch: 10, training loss: 83911.0234375\n",
      "16.656020641326904 seconds\n",
      "epoch: 11, training loss: 77797.2265625\n",
      "16.882617950439453 seconds\n",
      "epoch: 12, training loss: 30321.623046875\n",
      "17.082923412322998 seconds\n",
      "epoch: 13, training loss: 28491.13671875\n",
      "16.89716362953186 seconds\n",
      "epoch: 14, training loss: 62194.9609375\n",
      "16.662810564041138 seconds\n",
      "epoch: 15, training loss: 17840.326171875\n",
      "18.119270086288452 seconds\n",
      "epoch: 16, training loss: 33951.80078125\n",
      "16.91617774963379 seconds\n",
      "epoch: 17, training loss: 26109.84765625\n",
      "16.710687398910522 seconds\n",
      "epoch: 18, training loss: 51191.0\n",
      "16.591866493225098 seconds\n",
      "epoch: 19, training loss: 29747.056640625\n",
      "16.826912879943848 seconds\n",
      "epoch: 20, training loss: 24661.025390625\n",
      "16.663498878479004 seconds\n",
      "epoch: 21, training loss: 26683.146484375\n",
      "17.28489089012146 seconds\n",
      "epoch: 22, training loss: 25597.435546875\n",
      "16.4849271774292 seconds\n",
      "epoch: 23, training loss: 24516.28125\n",
      "16.93273901939392 seconds\n",
      "epoch: 24, training loss: 35391.7734375\n",
      "17.312286853790283 seconds\n",
      "epoch: 25, training loss: 33790.74609375\n",
      "17.135308980941772 seconds\n",
      "epoch: 26, training loss: 20099.47265625\n",
      "17.263417720794678 seconds\n",
      "epoch: 27, training loss: 25694.3046875\n",
      "17.066396474838257 seconds\n",
      "epoch: 28, training loss: 21557.21484375\n",
      "17.1261203289032 seconds\n",
      "epoch: 29, training loss: 29386.666015625\n",
      "17.346169471740723 seconds\n",
      "epoch: 30, training loss: 31199.912109375\n",
      "17.40972352027893 seconds\n",
      "epoch: 31, training loss: 29846.064453125\n",
      "17.21067523956299 seconds\n",
      "epoch: 32, training loss: 8541.0947265625\n",
      "17.223658323287964 seconds\n",
      "epoch: 33, training loss: 18042.388671875\n",
      "16.92403817176819 seconds\n",
      "epoch: 34, training loss: 19995.5546875\n",
      "17.734617710113525 seconds\n",
      "epoch: 35, training loss: 10785.798828125\n",
      "17.78870129585266 seconds\n",
      "epoch: 36, training loss: 14826.9365234375\n",
      "17.454641819000244 seconds\n",
      "epoch: 37, training loss: 8538.5927734375\n",
      "17.112972259521484 seconds\n",
      "epoch: 38, training loss: 6553.5087890625\n",
      "17.293917179107666 seconds\n",
      "epoch: 39, training loss: 16994.619140625\n",
      "17.465056896209717 seconds\n",
      "epoch: 40, training loss: 21650.89453125\n",
      "17.785186767578125 seconds\n",
      "epoch: 41, training loss: 18946.123046875\n",
      "18.996660470962524 seconds\n",
      "epoch: 42, training loss: 22830.208984375\n",
      "18.33464217185974 seconds\n",
      "epoch: 43, training loss: 18609.59375\n",
      "16.83226466178894 seconds\n",
      "epoch: 44, training loss: 19809.283203125\n",
      "16.15762996673584 seconds\n",
      "epoch: 45, training loss: 19030.462890625\n",
      "16.689415216445923 seconds\n",
      "epoch: 46, training loss: 21348.951171875\n",
      "16.858094453811646 seconds\n",
      "epoch: 47, training loss: 18336.525390625\n",
      "16.66242218017578 seconds\n",
      "epoch: 48, training loss: 18497.1875\n",
      "16.397663354873657 seconds\n",
      "epoch: 49, training loss: 17205.1875\n",
      "16.449008464813232 seconds\n",
      "epoch: 50, training loss: 19656.935546875\n",
      "16.753679037094116 seconds\n",
      "epoch: 51, training loss: 23018.9609375\n",
      "16.726607084274292 seconds\n",
      "epoch: 52, training loss: 23396.423828125\n",
      "16.00366473197937 seconds\n",
      "epoch: 53, training loss: 20382.68359375\n",
      "16.58109450340271 seconds\n",
      "epoch: 54, training loss: 8131.24365234375\n",
      "16.516080856323242 seconds\n",
      "epoch: 55, training loss: 10700.5673828125\n",
      "16.73365092277527 seconds\n",
      "epoch: 56, training loss: 7476.08154296875\n",
      "17.69419836997986 seconds\n",
      "epoch: 57, training loss: 10972.826171875\n",
      "16.943683862686157 seconds\n",
      "epoch: 58, training loss: 18679.66796875\n",
      "16.767679691314697 seconds\n",
      "epoch: 59, training loss: 18001.412109375\n",
      "17.200143337249756 seconds\n",
      "epoch: 60, training loss: 17267.00390625\n",
      "17.300122499465942 seconds\n",
      "epoch: 61, training loss: 23334.7421875\n",
      "17.333112716674805 seconds\n",
      "epoch: 62, training loss: 15732.0869140625\n",
      "16.299636125564575 seconds\n",
      "epoch: 63, training loss: 19700.755859375\n",
      "16.83855128288269 seconds\n",
      "epoch: 64, training loss: 19693.59765625\n",
      "16.24251699447632 seconds\n",
      "epoch: 65, training loss: 21293.5390625\n",
      "17.220017433166504 seconds\n",
      "epoch: 66, training loss: 16841.6015625\n",
      "16.736053943634033 seconds\n",
      "epoch: 67, training loss: 16594.5703125\n",
      "16.779215097427368 seconds\n",
      "epoch: 68, training loss: 20669.49609375\n",
      "17.00493288040161 seconds\n",
      "epoch: 69, training loss: 18336.4140625\n",
      "16.92006826400757 seconds\n",
      "epoch: 70, training loss: 16704.662109375\n",
      "17.024624347686768 seconds\n",
      "epoch: 71, training loss: 15674.6162109375\n",
      "16.77855610847473 seconds\n",
      "epoch: 72, training loss: 15604.06640625\n",
      "16.532074451446533 seconds\n",
      "epoch: 73, training loss: 15226.6142578125\n",
      "17.131641387939453 seconds\n",
      "epoch: 74, training loss: 16669.904296875\n",
      "16.965559482574463 seconds\n",
      "epoch: 75, training loss: 15676.0361328125\n",
      "16.803850889205933 seconds\n",
      "epoch: 76, training loss: 14945.6796875\n",
      "16.99685502052307 seconds\n",
      "epoch: 77, training loss: 16209.1796875\n",
      "16.665645599365234 seconds\n",
      "epoch: 78, training loss: 18548.9375\n",
      "16.279625177383423 seconds\n",
      "epoch: 79, training loss: 15810.9453125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3763/2223835105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgenerate_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3763/2838152407.py\u001b[0m in \u001b[0;36mgenerate_submission\u001b[0;34m(fp, model, opto, loss_fct, device, valid)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3763/1424538765.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, n_epochs, train_loader, loss_fct, criterion, device, val_loader, valid)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;31m# print('input: {}'.format(inp[0, :3]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;31m# print('preds: {}'.format(pred[0, :3]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3763/1424538765.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h, c)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m#print(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m#print(h[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m#out_all = torch.tensor(())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    680\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    681\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(input_size=2, hidden_size=512, output_size=2, device=device, num_layers=1).to(device)\n",
    "opto = torch.optim.AdamW(model.parameters(), lr=.001)\n",
    "loss_fct = nn.MSELoss()\n",
    "\n",
    "generate_submission('./submission.csv', model, opto, loss_fct, device, valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x[0,0] for x, y in train_loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5717c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8997907",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def fused_gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eccde5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
