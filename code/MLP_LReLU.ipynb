{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle \n",
    "from glob import glob\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", valid=False, normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "        \n",
    "        inps = []#np.array([])\n",
    "        out =  []#np.array([])\n",
    "        for i in range(inputs.shape[0]):\n",
    "            inp = np.concatenate([(inputs[i, :, 0] - inputs[i, 0, 0]).reshape(50, 1),\n",
    "                                  (inputs[i, :, 1] - inputs[i, 0, 1]).reshape(50, 1)], axis=1)\n",
    "            o = np.concatenate([(outputs[i, :, 0] - inputs[i, 0, 0]).reshape(60, 1),\n",
    "                                (outputs[i, :, 1] - inputs[i, 0, 1]).reshape(60, 1)], axis=1)\n",
    "            inps.append(inp)#np.append(inps,inp)\n",
    "            out.append(o)#np.append(out,o)\n",
    "            #inps = np.vstack((inps,inp))\n",
    "            #out = np.vstack((out,o))\n",
    "        \n",
    "        inps = np.array(inps)\n",
    "        out = np.array(out)\n",
    "             \n",
    "        if valid:\n",
    "            idx = int(len(inputs) * .8)\n",
    "            return inps[:idx], inps[idx:], out[:idx], out[idx:]\n",
    "        else:\n",
    "            return inps, out\n",
    "#   else: \n",
    "#         inps = []#np.array([])\n",
    "#         for i in range(inputs.shape[0]):\n",
    "#             inp = np.concatenate([(inputs[i, :, 0] - inputs[i, 0, 0]).reshape(50, 1), (inputs[i, :, 1] - inputs[i, 0, 1]).reshape(50, 1)], axis=1)\n",
    "#             inps.append(inp)\n",
    "#         inps = np.array(inps)\n",
    "\n",
    "    return inputs, None\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            data = (self.inputs[idx], self.outputs[idx])\n",
    "        else:\n",
    "            data = (self.inputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "    \n",
    "class ValidationDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, inputs, outputs, transform=None):\n",
    "        super(ValidationDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.inputs, self.outputs = inputs, outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'palo-alto' \n",
    "split = 'train'\n",
    "train_dataset  = ArgoverseDataset(city = city, split = split)\n",
    "\n",
    "##get_city_trajectories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 32  # batch size \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import random\n",
    "\n",
    "# def show_sample_batch(sample_batch):\n",
    "#     \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "#     inp, out = sample_batch\n",
    "#     batch_sz = inp.size(0)\n",
    "#     agent_sz = inp.size(1)\n",
    "    \n",
    "#     fig, axs = plt.subplots(1, batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "#     fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "#     axs = axs.ravel()   \n",
    "#     for i in range(batch_sz):\n",
    "#         axs[i].xaxis.set_ticks([])\n",
    "#         axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "#         axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "#         axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "\n",
    "        \n",
    "# for i_batch, sample_batch in enumerate(train_loader):\n",
    "#     # inp[i] is a scene with 50 coordinates, input[i, j] is a coordinate\n",
    "#     # gotta loop through each scene in the batch\n",
    "#     inp, out = sample_batch # inp: (batch size, 50, 2), out: (batch size, 60, 2)\n",
    "#     \"\"\"\n",
    "#     TODO:\n",
    "#       implement your Deep learning model\n",
    "#       implement training routine\n",
    "#     \"\"\"\n",
    "#     show_sample_batch(sample_batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe4eb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from d2l.ai\n",
    "def grad_clipping(net, theta):\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers, device, dropout=0):\n",
    "        super(RNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=n_layers, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "#         self.rnn.flatten_parameters()\n",
    "                        \n",
    "        out, (hn, cn) = self.rnn(x)\n",
    "        \n",
    "        print(out)\n",
    "        \n",
    "        o = hn[-1]\n",
    "        \n",
    "        print(self.fc(o))\n",
    "        print(self.fc(o).size())\n",
    "\n",
    "                        \n",
    "        return self.fc(out)\n",
    "    \n",
    "    \n",
    "import seaborn as sns\n",
    "def train(net, n_epochs, train_loader, loss_fct, criterion, device, val_loader=None, valid=False):\n",
    "    train_l= []\n",
    "    val_l = []    \n",
    "    for epoch in range(n_epochs):\n",
    "        ## training loop\n",
    "        for i_batch, batch in enumerate(train_loader):\n",
    "            # inp[i] is a scene with 50 coordinates, input[i, j] is a coordinate\n",
    "            inp, out = batch\n",
    "            inp = inp.float().to(device)\n",
    "            out = out.float().to(device)\n",
    "            \n",
    "            pred = net(inp).to(device)\n",
    "            # print('input: {}'.format(inp[0, :3]))\n",
    "            # print('preds: {}'.format(pred[0, :3]))\n",
    "            # print('true: {}'.format(out[0, :3]))\n",
    "            \n",
    "            loss = loss_fct(pred, out)\n",
    "\n",
    "            criterion.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            criterion.step()\n",
    "            \n",
    "        train_l.append(loss.item())\n",
    "        \n",
    "        if valid:\n",
    "            for i_batch, batch in enumerate(val_loader):\n",
    "                with torch.no_grad():\n",
    "                    inp, out = batch\n",
    "                    inp = inp.float().to(device)\n",
    "                    out = out.float().to(device)\n",
    "\n",
    "                    pred = net(inp).to(device)\n",
    "\n",
    "                    val_loss = loss_fct(pred, out)\n",
    "        if valid:\n",
    "            val_l.append(val_loss.item())\n",
    "            \n",
    "            print('epoch: {}, training loss: {}, validation loss: {}'.format(epoch + 1, loss, val_loss))\n",
    "        else:\n",
    "            print('epoch: {}, training loss: {}'.format(epoch + 1, loss))\n",
    "        \n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    sns.lineplot(ax=ax[0], x=np.arange(0, len(train_l)), y=train_l)\n",
    "    if valid:\n",
    "        sns.lineplot(ax=ax[1], x=np.arange(0, len(val_l)), y=val_l)\n",
    "    print('-'* 70)\n",
    "    return\n",
    "\n",
    "    \n",
    "def write_city_preds(net, test_loader, device, city, fp):       \n",
    "    scene = 0\n",
    "    output = ''\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i_batch, batch in enumerate(test_loader):\n",
    "            inp = batch\n",
    "            inp = inp.float().to(device)\n",
    "\n",
    "            first = inp[:, 0:1, :].clone().detach()\n",
    "\n",
    "            inp = inp - first\n",
    "            \n",
    "            preds = net(inp).to(device)\n",
    "            preds = preds + first\n",
    "            \n",
    "            flat = preds[0].flatten().cpu().tolist()\n",
    "            \n",
    "            row = ['{}_{}'.format(scene, city)] + flat\n",
    "            row = [str(i) for i in row]\n",
    "            output += ','.join(row) + '\\n'\n",
    "            \n",
    "            scene += 1\n",
    "    \n",
    "    try:\n",
    "        with open('./submission.csv', 'a') as f:\n",
    "            f.write(output)\n",
    "        print('Predictions for {} generated!'.format(city))\n",
    "        return 1\n",
    "    except:\n",
    "        print('Error! Unsuccessful write...')\n",
    "        return -1\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2222c",
   "metadata": {},
   "source": [
    "## These models were trained with SGD as the opto and it wasn't very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "582919b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(2, 256, 2).to(device)\n",
    "# opto = torch.optim.SGD(model.parameters(), lr=1)\n",
    "# loss_fct = nn.MSELoss()\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a200af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(2, 128, 2).to(device) \n",
    "# opto = torch.optim.SGD(model.parameters(), lr=1)\n",
    "# loss_fct = nn.MSELoss()\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea3aa6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(2, 128, 2).to(device)\n",
    "# opto = torch.optim.SGD(model.parameters(), lr=.1)\n",
    "# loss_fct = nn.MSELoss()\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f7c35",
   "metadata": {},
   "source": [
    "## Started using Adam optimizer / lstm with dropout, these next 2 models are usable, adding l2 loss didnt converge, currently using the second one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04ceae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(input_size=2, hidden_size=128, output_size=2, num_layers=2, dropout=.2).to(device)\n",
    "# opto = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "# loss_fct = nn.MSELoss()\n",
    "\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6d4aad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(input_size=2, hidden_size=256, output_size=2, num_layers=2, dropout=.2).to(device)\n",
    "# opto = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "# loss_fct = nn.MSELoss()\n",
    "\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183cd4f",
   "metadata": {},
   "source": [
    "## Bidirectional doesn't make sense here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b6cf9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(2, 256, 2, .2, True).to(device)\n",
    "# opto = torch.optim.Adam(model.parameters(), lr=.001, weight_decay=.01)\n",
    "# loss_fct = nn.MSELoss()\n",
    "\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a9ca7",
   "metadata": {},
   "source": [
    "# Test Dataset and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c1678689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(fp, model, n_epochs, batch_sz, opto, scheduler, loss_fct, device, valid=False):\n",
    "    cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"] \n",
    "    header = ['ID'] + ['v' + str(i) for i in range(0, 120)]\n",
    "    \n",
    "    with open(fp, 'w') as f:\n",
    "        f.write(','.join(header) + '\\n')\n",
    "\n",
    "    for city in cities:\n",
    "        if valid:\n",
    "            i, v_i, o, v_o = get_city_trajectories(city=city, split=\"train\", valid=valid)\n",
    "            training_data = ValidationDataset(i, o)\n",
    "            validation_data = ValidationDataset(v_i, v_o)\n",
    "            train_loader = DataLoader(training_data, batch_size=batch_sz)\n",
    "            val_loader = DataLoader(validation_data, batch_size=batch_sz)\n",
    "        else:\n",
    "            val_loader = None\n",
    "            training_data = ArgoverseDataset(city=city, split='train')\n",
    "            train_loader = DataLoader(training_data, batch_size=batch_sz)\n",
    "        \n",
    "#         train(model, n_epochs, train_loader, loss_fct, opto, device, val_loader, valid)\n",
    "        train_mlp(model, n_epochs, train_loader, loss_fct, opto, scheduler, device, city, val_loader, valid)\n",
    "        \n",
    "        #if not valid:\n",
    "        test_dataset  = ArgoverseDataset(city=city, split='test')\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "        write_city_preds(model, test_loader, device, city, fp)\n",
    "\n",
    "        print(\"\\nDone printing \" + str(city) + \" predictions\")\n",
    "        print('-'* 70)\n",
    "        \n",
    "    print(fp + ' generated!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "417c1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device):\n",
    "        super(MLP, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(100, 1024, device=device),\n",
    "            nn.LeakyReLU(negative_slope = 0.1, inplace = False),\n",
    "            nn.Linear(1024, 512, device=device),\n",
    "            nn.LeakyReLU(negative_slope = 0.1, inplace = False),\n",
    "            nn.Linear(512, 512, device=device)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(512, 1024, device=device),\n",
    "            nn.LeakyReLU(negative_slope = 0.1, inplace = False),\n",
    "            nn.Linear(1024, 120, device=device),\n",
    "            nn.LeakyReLU(negative_slope = 0.1, inplace = False),\n",
    "            nn.Linear(120, 120, device=device)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = x.reshape(-1, x.size(1) * x.size(2)).float()\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        #x = self.fc(x)\n",
    "        x = x.reshape(-1, 60, 2) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3c22622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, device):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.device = device\n",
    "        \n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(100, 64, device=device),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 32, device=device),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32, 32, device=device)\n",
    "#         )\n",
    "        \n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(32, 64, device=device),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 120, device=device),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(120, 120, device=device)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x.to(self.device)\n",
    "#         x = x.reshape(-1, x.size(1) * x.size(2)).float()\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.decoder(x)\n",
    "#         #x = self.fc(x)\n",
    "#         x = x.reshape(-1, 60, 2) \n",
    "#         return x\n",
    "\n",
    "\n",
    "def train_mlp(net, n_epochs, train_loader, loss_fct, criterion, scheduler, device, city, val_loader=None, valid=False):\n",
    "    train_l= []\n",
    "    val_l = []\n",
    "    for epoch in range(n_epochs):\n",
    "        ## training loop\n",
    "       # h, c = net.init_hidden(64)\n",
    "        for i_batch, batch in enumerate(train_loader):\n",
    "            inp, out = batch\n",
    "            inp = inp.float().to(device)\n",
    "            out = out.float().to(device)\n",
    "            \n",
    "            pred = net(inp).to(device)\n",
    "            \n",
    "            loss = loss_fct(pred, out)\n",
    "\n",
    "            criterion.zero_grad()\n",
    "            loss.backward()\n",
    "#             grad_clipping(net, 3)\n",
    "            criterion.step()\n",
    "        \n",
    "        train_l.append(loss.item())\n",
    "        \n",
    "        if valid:    \n",
    "            for i_batch, batch in enumerate(val_loader):\n",
    "                with torch.no_grad():\n",
    "                    inp, out = batch\n",
    "                    inp = inp.float().to(device)\n",
    "                    out = out.float().to(device)\n",
    "                    \n",
    "\n",
    "                    pred = net(inp).to(device)\n",
    "                    \n",
    "\n",
    "                    val_loss = loss_fct(pred, out)\n",
    "        if valid:\n",
    "            val_l.append(val_loss.item())\n",
    "            \n",
    "            print('epoch: {}, training loss: {}, validation loss: {}'.format(epoch + 1, loss, val_loss))\n",
    "        else:\n",
    "            print('epoch: {}, training loss: {}'.format(epoch + 1, loss))\n",
    "        \n",
    "        scheduler.step()\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    sns.lineplot(ax=ax[0], x=np.arange(0, len(train_l)), y=train_l).set_title(\"Loss (MSE) for \" + str(city))\n",
    "    \n",
    "    if valid:\n",
    "        sns.lineplot(ax=ax[1], x=np.arange(0, len(val_l)), y=val_l).set_title(\"Loss (MSE) for \" + str(city))\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ca970636",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e61f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, training loss: 54.17150115966797, validation loss: 51.24046325683594\n",
      "epoch: 2, training loss: 47.74026107788086, validation loss: 42.420963287353516\n",
      "epoch: 3, training loss: 43.30506896972656, validation loss: 35.39165115356445\n",
      "epoch: 4, training loss: 41.754310607910156, validation loss: 31.03917121887207\n",
      "epoch: 5, training loss: 42.25025939941406, validation loss: 30.07013702392578\n",
      "epoch: 6, training loss: 42.06145477294922, validation loss: 29.3631591796875\n",
      "epoch: 7, training loss: 41.055076599121094, validation loss: 28.413658142089844\n",
      "epoch: 8, training loss: 39.850704193115234, validation loss: 27.443971633911133\n",
      "epoch: 9, training loss: 39.17961502075195, validation loss: 26.70623207092285\n",
      "epoch: 10, training loss: 38.21807861328125, validation loss: 25.912538528442383\n",
      "epoch: 11, training loss: 37.1771125793457, validation loss: 25.112159729003906\n",
      "epoch: 12, training loss: 35.772212982177734, validation loss: 24.338829040527344\n",
      "epoch: 13, training loss: 34.55339431762695, validation loss: 23.77972412109375\n",
      "epoch: 14, training loss: 32.67424774169922, validation loss: 23.024734497070312\n",
      "epoch: 15, training loss: 32.29093933105469, validation loss: 22.832618713378906\n",
      "epoch: 16, training loss: 31.137447357177734, validation loss: 22.180883407592773\n",
      "epoch: 17, training loss: 31.494548797607422, validation loss: 22.1837100982666\n",
      "epoch: 18, training loss: 30.19344139099121, validation loss: 21.669570922851562\n",
      "epoch: 19, training loss: 29.695117950439453, validation loss: 21.413267135620117\n",
      "epoch: 20, training loss: 29.325105667114258, validation loss: 21.22951316833496\n",
      "epoch: 21, training loss: 29.093156814575195, validation loss: 21.124671936035156\n",
      "epoch: 22, training loss: 28.557743072509766, validation loss: 20.97538185119629\n",
      "epoch: 23, training loss: 28.257152557373047, validation loss: 20.778200149536133\n",
      "epoch: 24, training loss: 27.951284408569336, validation loss: 20.47087287902832\n",
      "epoch: 25, training loss: 27.634740829467773, validation loss: 20.186357498168945\n",
      "epoch: 26, training loss: 26.921199798583984, validation loss: 20.398771286010742\n",
      "epoch: 27, training loss: 26.703166961669922, validation loss: 20.312402725219727\n",
      "epoch: 28, training loss: 26.687931060791016, validation loss: 20.2432804107666\n",
      "epoch: 29, training loss: 26.648876190185547, validation loss: 20.080148696899414\n",
      "epoch: 30, training loss: 26.40494728088379, validation loss: 19.770116806030273\n",
      "epoch: 31, training loss: 26.68701934814453, validation loss: 19.934606552124023\n",
      "epoch: 32, training loss: 26.410234451293945, validation loss: 19.83708953857422\n",
      "epoch: 33, training loss: 26.239572525024414, validation loss: 19.74531364440918\n",
      "epoch: 34, training loss: 26.21146583557129, validation loss: 19.838825225830078\n",
      "epoch: 35, training loss: 25.934717178344727, validation loss: 19.617149353027344\n",
      "epoch: 36, training loss: 25.89139747619629, validation loss: 19.375993728637695\n",
      "epoch: 37, training loss: 25.601375579833984, validation loss: 19.306915283203125\n",
      "epoch: 38, training loss: 25.294506072998047, validation loss: 19.195703506469727\n",
      "epoch: 39, training loss: 25.324886322021484, validation loss: 19.088092803955078\n",
      "epoch: 40, training loss: 25.415966033935547, validation loss: 19.08995819091797\n",
      "epoch: 41, training loss: 25.283544540405273, validation loss: 18.966215133666992\n",
      "epoch: 42, training loss: 25.10885238647461, validation loss: 18.899795532226562\n"
     ]
    }
   ],
   "source": [
    "#model = RNN(input_size=2, hidden_size=32, output_size=2, n_layers=2, device=device, dropout=.2).to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model =  MLP(2,128,2, device).to(device)#MLP(input_size=100, hidden_size=128, output_size=120, device=device).to(device)#Regression().to(device)\n",
    "opto = torch.optim.Adam(model.parameters(), lr = .0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opto, 25 , gamma=0.9)\n",
    "loss_fct = nn.MSELoss()\n",
    " \n",
    "generate_submission('./submission.csv', model, 200, 64, opto, scheduler, loss_fct, device, valid=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
