{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", valid=False, normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "        \n",
    "        if valid:\n",
    "            idx = int(len(inputs) * .8)\n",
    "            return inputs[:idx], inputs[idx:], outputs[:idx], outputs[idx:]\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            data = (self.inputs[idx], self.outputs[idx])\n",
    "        else:\n",
    "            data = (self.inputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "    \n",
    "class ValidationDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, inputs, outputs, transform=None):\n",
    "        super(ValidationDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.inputs, self.outputs = inputs, outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'palo-alto' \n",
    "split = 'train'\n",
    "train_dataset  = ArgoverseDataset(city = city, split = split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 32  # batch size \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import random\n",
    "\n",
    "# def show_sample_batch(sample_batch):\n",
    "#     \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "#     inp, out = sample_batch\n",
    "#     batch_sz = inp.size(0)\n",
    "#     agent_sz = inp.size(1)\n",
    "    \n",
    "#     fig, axs = plt.subplots(1, batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "#     fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "#     axs = axs.ravel()   \n",
    "#     for i in range(batch_sz):\n",
    "#         axs[i].xaxis.set_ticks([])\n",
    "#         axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "#         axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "#         axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "\n",
    "        \n",
    "# for i_batch, sample_batch in enumerate(train_loader):\n",
    "#     # inp[i] is a scene with 50 coordinates, input[i, j] is a coordinate\n",
    "#     # gotta loop through each scene in the batch\n",
    "#     inp, out = sample_batch # inp: (batch size, 50, 2), out: (batch size, 60, 2)\n",
    "#     \"\"\"\n",
    "#     TODO:\n",
    "#       implement your Deep learning model\n",
    "#       implement training routine\n",
    "#     \"\"\"\n",
    "#     show_sample_batch(sample_batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe4eb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from d2l.ai\n",
    "def grad_clipping(net, theta):\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device, num_layers=1, dropout=0, bidirectional=False):\n",
    "        super(RNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        \n",
    "        x = F.layer_norm(x, x.size())\n",
    "        \n",
    "        out, _ = self.rnn(x)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "import seaborn as sns\n",
    "def train(net, n_epochs, train_loader, loss_fct, criterion, device, val_loader=None, valid=False):\n",
    "    train_l= []\n",
    "    val_l = []\n",
    "    for epoch in range(n_epochs):\n",
    "        ## training loop\n",
    "        for i_batch, batch in enumerate(train_loader):\n",
    "            # inp[i] is a scene with 50 coordinates, input[i, j] is a coordinate\n",
    "            inp, out = batch\n",
    "            inp = inp.float().to(device)\n",
    "            out = out.float().to(device)\n",
    "            \n",
    "            # 0 pad the end of input seq\n",
    "            inp = torch.cat((inp, torch.zeros(inp.size(0), 10, inp.size(2), device=device)), dim=1)\n",
    "            pred = net(inp).to(device)\n",
    "            # print('input: {}'.format(inp[0, :3]))\n",
    "            # print('preds: {}'.format(pred[0, :3]))\n",
    "            # print('true: {}'.format(out[0, :3]))\n",
    "            \n",
    "            loss = loss_fct(pred, out)\n",
    "\n",
    "            criterion.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            criterion.step()\n",
    "            \n",
    "        train_l.append(loss.item())\n",
    "        \n",
    "        if valid:\n",
    "            for i_batch, batch in enumerate(val_loader):\n",
    "                with torch.no_grad():\n",
    "                    inp, out = batch\n",
    "                    inp = inp.float().to(device)\n",
    "                    out = out.float().to(device)\n",
    "\n",
    "                    inp = torch.cat((inp, torch.zeros(inp.size(0), 10, inp.size(2), device=device)), dim=1)\n",
    "                    pred = net(inp).to(device)\n",
    "\n",
    "                    val_loss = loss_fct(pred, out)\n",
    "        if valid:\n",
    "            val_l.append(val_loss.item())\n",
    "            \n",
    "            print('epoch: {}, training loss: {}, validation loss: {}'.format(epoch + 1, loss, val_loss))\n",
    "        else:\n",
    "            print('epoch: {}, training loss: {}'.format(epoch + 1, loss))\n",
    "        \n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    sns.lineplot(ax=ax[0], x=np.arange(0, len(train_l)), y=train_l)\n",
    "    if valid:\n",
    "        sns.lineplot(ax=ax[1], x=np.arange(0, len(val_l)), y=val_l)\n",
    "    print('-'* 70)\n",
    "    return\n",
    "\n",
    "# def predict(net, data_loader, device):\n",
    "#     with torch.no_grad():\n",
    "#         for i_batch, batch in enumerate(data_loader):\n",
    "#             inp, _ = batch\n",
    "#             inp = torch.cat((inp.float().to(device), torch.zeros(inp.size(0), 10, inp.size(2)).to(device)), dim=1)\n",
    "                        \n",
    "#             preds = net(inp.float().to(device))\n",
    "            \n",
    "#             return preds\n",
    "        \n",
    "def write_city_preds(net, test_loader, device, city, fp):       \n",
    "    scene = 0\n",
    "    output = ''\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i_batch, batch in enumerate(test_loader):\n",
    "            inp = batch\n",
    "            inp = inp.float().to(device)\n",
    "            inp = torch.cat((inp, torch.zeros(inp.size(0), 10, inp.size(2), device=device)), dim=1)\n",
    "\n",
    "            preds = net(inp)\n",
    "            flat = preds[0].flatten().cpu().tolist()\n",
    "            \n",
    "            row = ['{}_{}'.format(scene, city)] + flat\n",
    "            row = [str(i) for i in row]\n",
    "            output += ','.join(row) + '\\n'\n",
    "            \n",
    "            scene += 1\n",
    "    \n",
    "    try:\n",
    "        with open('./submission.csv', 'a') as f:\n",
    "            f.write(output)\n",
    "        print('Predictions for {} generated!'.format(city))\n",
    "        return 1\n",
    "    except:\n",
    "        print('Error! Unsuccessful write...')\n",
    "        return -1\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2222c",
   "metadata": {},
   "source": [
    "## These models were trained with SGD as the opto and it wasn't very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582919b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(2, 256, 2).to(device)\n",
    "# opto = torch.optim.SGD(model.parameters(), lr=1)\n",
    "# loss_fct = nn.MSELoss()\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a200af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(2, 128, 2).to(device)\n",
    "# opto = torch.optim.SGD(model.parameters(), lr=1)\n",
    "# loss_fct = nn.MSELoss()\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea3aa6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(2, 128, 2).to(device)\n",
    "# opto = torch.optim.SGD(model.parameters(), lr=.1)\n",
    "# loss_fct = nn.MSELoss()\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f7c35",
   "metadata": {},
   "source": [
    "## Started using Adam optimizer / lstm with dropout, these next 2 models are usable, adding l2 loss didnt converge, currently using the second one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ceae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(input_size=2, hidden_size=128, output_size=2, num_layers=2, dropout=.2).to(device)\n",
    "# opto = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "# loss_fct = nn.MSELoss()\n",
    "\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4aad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(input_size=2, hidden_size=256, output_size=2, num_layers=2, dropout=.2).to(device)\n",
    "# opto = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "# loss_fct = nn.MSELoss()\n",
    "\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183cd4f",
   "metadata": {},
   "source": [
    "## Bidirectional doesn't make sense here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6cf9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(2, 256, 2, .2, True).to(device)\n",
    "# opto = torch.optim.Adam(model.parameters(), lr=.001, weight_decay=.01)\n",
    "# loss_fct = nn.MSELoss()\n",
    "\n",
    "# train(model, 100, train_loader, opto, loss_fct, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a9ca7",
   "metadata": {},
   "source": [
    "# Test Dataset and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1678689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(fp, model, opto, loss_fct, device, valid=False):\n",
    "    cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"] \n",
    "    header = ['ID'] + ['v' + str(i) for i in range(0, 120)]\n",
    "    \n",
    "    with open(fp, 'w') as f:\n",
    "        f.write(','.join(header) + '\\n')\n",
    "\n",
    "    for city in cities:\n",
    "        batch_sz = 64\n",
    "        if valid:\n",
    "            i, v_i, o, v_o = get_city_trajectories(city=city, split=\"train\", valid=valid)\n",
    "            training_data = ValidationDataset(i, o)\n",
    "            validation_data = ValidationDataset(v_i, v_o)\n",
    "            train_loader = DataLoader(training_data, batch_size=batch_sz)\n",
    "            val_loader = DataLoader(validation_data, batch_size=batch_sz)\n",
    "        else:\n",
    "            val_loader = None\n",
    "            training_data = ArgoverseDataset(city=city, split='train')\n",
    "            train_loader = DataLoader(training_data, batch_size=batch_sz)\n",
    "        \n",
    "        train(model, 150, train_loader, loss_fct, opto, device, val_loader, valid)\n",
    "        \n",
    "        if not valid:\n",
    "            test_dataset  = ArgoverseDataset(city=city, split='test')\n",
    "            test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "            write_city_preds(model, test_loader, device, city, fp)\n",
    "\n",
    "    print(fp + ' generated!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362cf69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, training loss: 1731522.5\n",
      "epoch: 2, training loss: 1589051.75\n",
      "epoch: 3, training loss: 1462481.25\n",
      "epoch: 4, training loss: 1348537.75\n",
      "epoch: 5, training loss: 1245750.625\n",
      "epoch: 6, training loss: 1152664.5\n",
      "epoch: 7, training loss: 1071228.0\n",
      "epoch: 8, training loss: 998954.75\n",
      "epoch: 9, training loss: 932759.9375\n",
      "epoch: 10, training loss: 872911.4375\n",
      "epoch: 11, training loss: 820262.9375\n",
      "epoch: 12, training loss: 772294.75\n",
      "epoch: 13, training loss: 728069.0625\n",
      "epoch: 14, training loss: 689325.0\n",
      "epoch: 15, training loss: 654535.5\n",
      "epoch: 16, training loss: 620733.3125\n",
      "epoch: 17, training loss: 590793.3125\n",
      "epoch: 18, training loss: 560846.375\n",
      "epoch: 19, training loss: 533717.1875\n",
      "epoch: 20, training loss: 508160.28125\n",
      "epoch: 21, training loss: 484455.46875\n",
      "epoch: 22, training loss: 462215.28125\n",
      "epoch: 23, training loss: 440940.625\n",
      "epoch: 24, training loss: 418121.25\n",
      "epoch: 25, training loss: 396001.6875\n",
      "epoch: 26, training loss: 375554.59375\n",
      "epoch: 27, training loss: 355326.40625\n",
      "epoch: 28, training loss: 336303.4375\n",
      "epoch: 29, training loss: 318630.6875\n",
      "epoch: 30, training loss: 300857.1875\n",
      "epoch: 31, training loss: 285592.4375\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(input_size=2, hidden_size=256, output_size=2, device=device, num_layers=2, dropout=.2).to(device)\n",
    "opto = torch.optim.Adam(model.parameters(), lr=.0005)\n",
    "loss_fct = nn.MSELoss()\n",
    "\n",
    "generate_submission('./submission.csv', model, opto, loss_fct, device, valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d681fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
